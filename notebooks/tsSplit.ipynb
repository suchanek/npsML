{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/suchanek/repos/npsML/notebooks/tsSplit.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/tsSplit.ipynb#ch0000001?line=22'>23</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshutil\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/tsSplit.ipynb#ch0000001?line=24'>25</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/tsSplit.ipynb#ch0000001?line=25'>26</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhub\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/tsSplit.ipynb#ch0000001?line=26'>27</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_text\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtext\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/tsSplit.ipynb#ch0000001?line=27'>28</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mofficial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnlp\u001b[39;00m \u001b[39mimport\u001b[39;00m optimization  \u001b[39m# to create AdamW optimizer\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Bestbuy specifics\n",
    "import bby\n",
    "import bby.util as ut\n",
    "\n",
    "from bby.util import clean_doc, tb_enrich, nps_cleanstring\n",
    "from bby.util import detokenize\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "null_str = \"xyxyxz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the comment into a form suitable for tokenizing\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(text_to_word_sequence(sentence))\n",
    "    return\n",
    "\n",
    "def str_it(_ls):\n",
    "    ls = str(_ls)\n",
    "    word_tokens = word_tokenize(ls)\n",
    "    ls = [w for w in word_tokens]\n",
    "\n",
    "    ls = \" \".join(ls)\n",
    "    return ls\n",
    "\n",
    "def write_txt_files(sentencelist, prefix='nps'):\n",
    "    lc = 1\n",
    "    print(f'Writing: {len(sentencelist)} files...')\n",
    "    for item in sentencelist:\n",
    "        outfilename = f'{prefix}_{lc}.txt'\n",
    "        try:\n",
    "            outfile = open(outfilename, 'w')\n",
    "        except OSError as error:\n",
    "            print(f'Cannot create file: {outfilename}. Failed with error: {error}! Exiting')\n",
    "            return\n",
    "        outfile.write(item)\n",
    "        outfile.write('\\n')\n",
    "        lc += 1\n",
    "    outfile.close()\n",
    "    return\n",
    "\n",
    "def iterative_train_test_split_dataframe(X, y, test_size):\n",
    "    df_index = np.expand_dims(X.index.to_numpy(), axis=1)\n",
    "    X_train, y_train, X_test, y_test = iterative_train_test_split(df_index, y, test_size = test_size)\n",
    "    X_train = X.loc[X_train[:,0]]\n",
    "    X_test = X.loc[X_test[:,0]]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directories for train/test split of nps data in the format needed by\n",
    "# keras read_text_dataset\n",
    "#\n",
    "\n",
    "def write_NPS_text_dataset(input_filename, rootpath='npsdata', frac=0.2):\n",
    "    curr_dir = os.getcwd()\n",
    "    data_dir = \"/users/suchanek/repos/npsml/data/clean\"\n",
    "    os.chdir(data_dir)\n",
    "    class_list = ['detractor', 'passive', 'promoter']\n",
    "\n",
    "    # try to read the input file\n",
    "    try:\n",
    "        os.path.exists(input_filename)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "        print(f'Cant read input file {input_filename}. Fatal, exiting')\n",
    "        return\n",
    "    \n",
    "    NPS_df = pd.read_csv(input_filename, index_col='respid2')\n",
    "    \n",
    "    # make the train and test subdirs for trainpath and testpath\n",
    "    trainpath = os.path.join(rootpath,'train')\n",
    "    testpath = os.path.join(rootpath,'test')\n",
    "    # print(f'train: {trainpath}, test: {testpath}')\n",
    "\n",
    "    # remove the entire tree first\n",
    "    try:\n",
    "        os.chdir(data_dir)\n",
    "        shutil.rmtree(rootpath)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "\n",
    "    try: \n",
    "        os.makedirs(trainpath) \n",
    "    except OSError as error: \n",
    "        print(error)  \n",
    "\n",
    "    try: \n",
    "        os.makedirs(testpath) \n",
    "    except OSError as error: \n",
    "        print(error)  \n",
    "    \n",
    "    # now make the class subdirectories for training and testing\n",
    "    os.chdir(data_dir)\n",
    "    os.chdir(trainpath)\n",
    "    for cls in class_list:\n",
    "        os.mkdir(cls)\n",
    "    \n",
    "    os.chdir(data_dir)\n",
    "    os.chdir(testpath)\n",
    "    for cls in class_list:\n",
    "        os.mkdir(cls)\n",
    "    \n",
    "    prom_list_mask = NPS_df['NPS_Code'] == 2\n",
    "    pass_list_mask = NPS_df['NPS_Code'] == 1\n",
    "    det_list_mask = NPS_df['NPS_Code'] == 0\n",
    "\n",
    "    # nps_list = NPS_df['NPSCommentCleaned'].apply(str_it)\n",
    "\n",
    "    prom_list = NPS_df[prom_list_mask]\n",
    "    pass_list = NPS_df[pass_list_mask]\n",
    "    det_list = NPS_df[det_list_mask]\n",
    "\n",
    "    prom_list_len = prom_list.shape[0]\n",
    "    pass_list_len = pass_list.shape[0]\n",
    "    det_list_len = det_list.shape[0]\n",
    "\n",
    "    len_list = [prom_list_len, pass_list_len, det_list_len]\n",
    "    print (f'Overall Distribution: Promoters: {prom_list_len}, Passives: {pass_list_len}, Detractors: {det_list_len}')\n",
    "\n",
    "    shortest = np.argmin(len_list)\n",
    "    sample_size = int(np.round(len_list[shortest] * frac))\n",
    "    print(sample_size)\n",
    "\n",
    "    prom_sample_size = int(np.round(prom_list_len * frac))\n",
    "    pass_sample_size = int(np.round(pass_list_len * frac))\n",
    "    det_sample_size = int(np.round(det_list_len * frac))\n",
    "\n",
    "    # these subsets represent the test subset\n",
    "    prom_list_test = prom_list.sample(sample_size)\n",
    "    pass_list_test = pass_list.sample(sample_size)\n",
    "    det_list_test = det_list.sample(sample_size)\n",
    "\n",
    "    prom_list_train = prom_list[~prom_list.apply(tuple,1).isin(prom_list_test.apply(tuple, 1))]\n",
    "    pass_list_train = pass_list[~pass_list.apply(tuple,1).isin(pass_list_test.apply(tuple, 1))]\n",
    "    det_list_train = det_list[~det_list.apply(tuple,1).isin(det_list_test.apply(tuple, 1))]\n",
    "\n",
    "    prom_train_sent = prom_list_train['NPSCommentCleaned'].apply(str_it)\n",
    "    prom_test_sent = prom_list_test['NPSCommentCleaned'].apply(str_it)\n",
    "\n",
    "    pass_train_sent = pass_list_train['NPSCommentCleaned'].apply(str_it)\n",
    "    pass_test_sent = pass_list_test['NPSCommentCleaned'].apply(str_it)\n",
    "\n",
    "    det_train_sent = det_list_train['NPSCommentCleaned'].apply(str_it)\n",
    "    det_test_sent = det_list_test['NPSCommentCleaned'].apply(str_it)\n",
    "\n",
    "    print(f'Promoters:')\n",
    "    print(f'Training size: {prom_list_train.shape[0]}')\n",
    "    print(f'Testing size {prom_list_test.shape[0]}')\n",
    "    print(f'Original size: {prom_list.shape[0]}')\n",
    "        \n",
    "    print(f'\\nPassives:')\n",
    "    print(f'Training size: {pass_list_train.shape[0]}')\n",
    "    print(f'Testing size {pass_list_test.shape[0]}')\n",
    "    print(f'Original size: {pass_list.shape[0]}')\n",
    "    \n",
    "    print(f'\\nDetractors:')\n",
    "    print(f'Training size: {det_list_train.shape[0]}')\n",
    "    print(f'Testing size {det_list_test.shape[0]}')\n",
    "    print(f'Original size: {det_list.shape[0]}')\n",
    "    \n",
    "    # Checking balance of target classes after equalization\n",
    "    sentiments = list(NPS_df[\"NPS® Breakdown\"].unique())\n",
    "    sentiment_nums = [len(NPS_df[NPS_df[\"NPS® Breakdown\"] == sentiment]) / len(NPS_df) for sentiment in sentiments]\n",
    "\n",
    "    print (f'Overall Distribution: Promoters: {prom_list_len}, Passives: {pass_list_len}, Detractors: {det_list_len}')\n",
    "    plt.bar(sentiments, sentiment_nums)\n",
    "\n",
    "    # now write the training files by class\n",
    "    os.chdir(data_dir)\n",
    "    os.chdir(trainpath)\n",
    "    os.chdir('promoter')\n",
    "    write_txt_files(prom_train_sent)\n",
    "    os.chdir('..')\n",
    "    os.chdir('passive')\n",
    "    write_txt_files(pass_train_sent)\n",
    "    os.chdir('..')\n",
    "    os.chdir('detractor')\n",
    "    write_txt_files(det_train_sent)\n",
    "    \n",
    "    # now write testing files by class\n",
    "    os.chdir(data_dir)\n",
    "    os.chdir(testpath)\n",
    "    os.chdir('promoter')\n",
    "    write_txt_files(prom_test_sent)\n",
    "    os.chdir('..')\n",
    "    os.chdir('passive')\n",
    "    write_txt_files(pass_test_sent)\n",
    "    os.chdir('..')\n",
    "    os.chdir('detractor')\n",
    "    write_txt_files(det_test_sent)\n",
    "    \n",
    "    os.chdir(curr_dir)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'current: {os.getcwd()}')\n",
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Distribution: Promoters: 10131, Passives: 8881, Detractors: 11381\n",
      "1776\n",
      "Promoters:\n",
      "Training size: 8352\n",
      "Testing size 1776\n",
      "Original size: 10131\n",
      "\n",
      "Passives:\n",
      "Training size: 7105\n",
      "Testing size 1776\n",
      "Original size: 8881\n",
      "\n",
      "Detractors:\n",
      "Training size: 9605\n",
      "Testing size 1776\n",
      "Original size: 11381\n",
      "Overall Distribution: Promoters: 10131, Passives: 8881, Detractors: 11381\n",
      "Writing: 8352 files...\n",
      "Writing: 7105 files...\n",
      "Writing: 9605 files...\n",
      "Writing: 1776 files...\n",
      "Writing: 1776 files...\n",
      "Writing: 1776 files...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir('/users/suchanek/repos/npsML/data/clean')\n",
    "os.listdir()\n",
    "write_NPS_text_dataset('NPS_NATL_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "os.chdir('/users/suchanek/repos/npsML/data/clean')\n",
    "\n",
    "NPS_df = pd.read_csv('NPS_NATL_subset.csv', index_col='respid2', usecols=['respid2', 'NPS_Code', 'NPSCommentCleaned'])\n",
    "y = NPS_df['NPS_Code'].values.tolist()\n",
    "y = np.array(y)\n",
    "labels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\n",
    "del y\n",
    "\n",
    "np.random.seed(42)\n",
    "X_train_df, y_train_df, X_test_df, y_test_df = iterative_train_test_split_dataframe(NPS_df, labels, 0.5)\n",
    "\n",
    "\n",
    "#NPS_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25062 files belonging to 3 classes.\n",
      "Using 20050 files for training.\n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 14:09:36.206805: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-13 14:09:36.207254: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25062 files belonging to 3 classes.\n",
      "Using 5012 files for validation.\n",
      "Found 5328 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "curr_dir = os.getcwd()\n",
    "data_dir = \"/users/suchanek/repos/npsml/data/clean\"\n",
    "class_list = ['detractor', 'passive', 'promoter']\n",
    "\n",
    "os.chdir(data_dir)\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'npsdata/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'npsdata/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'npsdata/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: b'because they took days to look my computer\\n'\n",
      "Label : 1 (passive)\n",
      "Comment: b'we had to drive miles to take my desktop to the best buy in denton can not send emails and webroot is not scanning my computer neither problem was resolved at the store now have to wait to have technician come to my house when we placed the original phone call we asked to have technician come to the house but we were told we needed to bring the computer to the store\\n'\n",
      "Label : 1 (passive)\n",
      "Comment: b'they react like robots\\n'\n",
      "Label : 0 (detractor)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 14:11:21.540275: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-07-13 14:11:21.565884: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Comment: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-03-29T12:30:15.782203Z",
     "iopub.status.busy": "2022-03-29T12:30:15.782025Z",
     "iopub.status.idle": "2022-03-29T12:30:15.792580Z",
     "shell.execute_reply": "2022-03-29T12:30:15.792025Z"
    },
    "id": "y8_ctG55-uTX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e9fb08708a348826ea2596edb892bc9ad5e5b82e2f5c7730c1edba9ecbcf32d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
