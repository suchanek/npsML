{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Purpose: clean up raw NPS .xlsx files prior to training, write cleaned output to new .csv file\n",
    "# Method: \n",
    "#   1) For each store... Remove missing values, stop words\n",
    "#   2) Use textblob to calculate sentiment scores for NPS and Overall comments\n",
    "#   3) Add sentiment scores for both the NPS Comment, and Overall Comment.\n",
    "#   4) Clean text, remove punctuation, lemmatize words\n",
    "#   5) Combine all store comments into single aggregated file by district\n",
    "#\n",
    "# Author: Eric G. Suchanek, PhD\n",
    "# (c)2022 BestBuy, all rights reserved\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bby module loaded\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#TextBlob Features\n",
    "from textblob import TextBlob\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Bestbuy specifics\n",
    "from bby import bby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_data(data):\n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(\"\\'\", \"\", data)\n",
    "\n",
    "    # Remove elipsis\n",
    "    data = re.sub(\"\\...\",  \" \", data)\n",
    "\n",
    "    # Strip escaped quotes\n",
    "    data = re.sub('\\\\\"', '', data)\n",
    " \n",
    "    # Strip quotes\n",
    "    data = re.sub('\"', '', data)\n",
    "        \n",
    "    return data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(sent):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(sent)\n",
    " \n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    " \n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return(filtered_sentence)\n",
    "# Adding sentiment dimensions with textblob\n",
    "def tb_enrich(ls):\n",
    "    #Enriches a column of text with TextBlob Sentiment Analysis outputs\n",
    "    tb_polarity = []\n",
    "    tb_subject = []\n",
    "\n",
    "    for comment in ls:\n",
    "        sentiment = TextBlob(comment).sentiment\n",
    "        tb_polarity.append(sentiment[0])\n",
    "        tb_subject.append(sentiment[1])\n",
    "    return tb_polarity, tb_subject\n",
    "# \n",
    "# write the list of comments to a file based on the path, district, and prefix\n",
    "def write_sentences(sentencelist, storename, _path, district, prefix):\n",
    "    lc = 1\n",
    "    outfilename = f'{_path}{district}_{storename}_{prefix}_{lc}{bby._txt_extension}'\n",
    "    outfile = open(outfilename, 'w')\n",
    "\n",
    "    for items in sentencelist:\n",
    "        outfile.writelines(items)\n",
    "        outfile.write('\\n')\n",
    "        lc += 1\n",
    "    outfile.close()\n",
    "    return\n",
    "# convert 'promoter' 'passive' 'detractor' to numerical index\n",
    "def nps_to_code(nps_list):\n",
    "    codelist = []\n",
    "    for comment in nps_list:\n",
    "        if (comment ==\"Promoter\"):\n",
    "            codelist.append('2')\n",
    "        elif (comment == \"Passive\"):\n",
    "            codelist.append('1')\n",
    "        elif (comment == \"Detractor\"):\n",
    "            codelist.append('0')\n",
    "        else:\n",
    "            codelist.append('xxx')\n",
    "    return(codelist)\n",
    "        \n",
    "# write the comments (nps and overall) to files in specific directories based on whether they are\n",
    "# promoters, passive or detractors\n",
    "\n",
    "def write_comments(district, storelist):\n",
    "    DEBUG = bby.DEBUG\n",
    "\n",
    "    if (DEBUG):\n",
    "        print(storelist)\n",
    "\n",
    "    for storename in storelist:\n",
    "        if (DEBUG):\n",
    "            print(f'....Writing comments for store: {storename}')\n",
    "        input_filename = f\"{bby._cleaned_path}{bby._output_filename_prefix}{storename}_{district}.csv\"\n",
    "        new_df = pd.read_csv(input_filename)\n",
    "\n",
    "        pos_comments = new_df.loc[new_df['NPS_Code'] == 2]\n",
    "        pass_comments = new_df.loc[new_df['NPS_Code'] == 1]\n",
    "        detr_comments = new_df.loc[new_df['NPS_Code'] == 0]\n",
    "\n",
    "        pos_stringlist = pos_comments['NPSCommentCleaned']\n",
    "        pass_stringlist = pass_comments['NPSCommentCleaned']\n",
    "        detr_stringlist = detr_comments['NPSCommentCleaned']\n",
    "        \n",
    "        write_sentences(pos_stringlist, storename, bby._prom_words_path, district, \"NPSComment\")\n",
    "        write_sentences(pass_stringlist, storename, bby._pass_words_path, district, \"NPSComment\")\n",
    "        write_sentences(detr_stringlist, storename, bby._det_words_path, district, \"NPSComment\")\n",
    "\n",
    "        pos_stringlist = pos_comments['OverallCommentCleaned']\n",
    "        pass_stringlist = pass_comments['OverallCommentCleaned']\n",
    "        detr_stringlist = detr_comments['OverallCommentCleaned']\n",
    "\n",
    "        write_sentences(pos_stringlist, storename, bby._prom_words_path, district, \"OverallComment\")\n",
    "        write_sentences(pass_stringlist, storename, bby._pass_words_path, district, \"OverallComment\")\n",
    "        write_sentences(detr_stringlist, storename, bby._det_words_path, district, \"OverallComment\")\n",
    "    return\n",
    "# NPS spreadsheet cleanup and reformatting for natural language processing\n",
    "# data paths and filename patterns\n",
    "# Assumes following directory structure\n",
    "# main directory/\n",
    "#  - notebook/\n",
    "#  - data/\n",
    "#  - raw/\n",
    "#   -clean/\n",
    "\n",
    "# Given the district, read the raw .xlsx file, fill null values, and perform the following cleanup:\n",
    "#  - remove URL, emails, extraneous punctuation\n",
    "#  - calculate sentiment scores from the blob package\n",
    "# write a new .csv file with NPS comment, overall comment, confirm number, location, workforce, NPS rating\n",
    "#\n",
    "from bby import bby\n",
    "_market = bby.Our_Market\n",
    "_district = bby.Our_District\n",
    "\n",
    "def NPS_cleanup(market=_market, district=_district):\n",
    "    print(f'Processing market: {market}')\n",
    "    NPS_cleanup_district(market, district)\n",
    "    return\n",
    "\n",
    "\n",
    "def NPS_cleanup_district(market, district):   \n",
    "    storelist1 = bby.district_stores_dict.get(market)\n",
    "    storelist = storelist1.get(district)\n",
    "    _cleaned_path = bby._cleaned_path\n",
    "    _raw_path = bby._raw_path\n",
    "    _output_filename_prefix = bby._output_filename_prefix\n",
    "    _filename_prefix = bby._filename_prefix\n",
    "    \n",
    "    # filename for aggregated final file\n",
    "    cleaned_filename = f\"{_cleaned_path}NPS_District_{district}.csv\"\n",
    "    output_filename_list = []\n",
    "\n",
    "    print(f'..Processing District: {district}')\n",
    "    \n",
    "    for storename in storelist:\n",
    "        all_promotors_list = []\n",
    "        all_passive_list = []\n",
    "        all_detractor_list = []\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "        input_filename = f\"{_raw_path}{_filename_prefix}{storename}.xlsx\"\n",
    "        output_filename = f\"{_cleaned_path}{_output_filename_prefix}{storename}_{district}.csv\"\n",
    "        output_filename_list.append(output_filename)\n",
    "        storename_string = f'_{storename}'\n",
    "\n",
    "        # read the .xlsx file, skipping the first 4 lines since they are not the\n",
    "        # actual column headers\n",
    "        print(f'...Processing Store: {storename}')\n",
    "        all_df = pd.read_excel(input_filename, header=3)\n",
    "        \n",
    "        # Fill null values.\n",
    "        all_df['NPS® Comment'].fillna(\"NONE\", inplace = True)\n",
    "        all_df['Overall Comment'].fillna(\"NONE\", inplace = True)\n",
    "        all_df['ConfirmationNumber'].fillna(\"0000\", inplace = True)\n",
    "        all_df['Service Order ID'].fillna(\"0000-0000\", inplace = True)\n",
    "        all_df['Location'] = storename_string\n",
    "\n",
    "        NPS_code = nps_to_code(all_df['NPS® Breakdown'].values.tolist())\n",
    "        \n",
    "        new_df = all_df[['Location','Workforce','NPS® Breakdown']].copy()\n",
    "        new_df['NPS_Code'] = NPS_code\n",
    "\n",
    "        # Map the 'promoter', 'passive', 'detractor' strings to 2, 1, 0 in order to encode the\n",
    "        # NPS overall sentiment.\n",
    "        # Define how we want to change the label name\n",
    "        \n",
    "        label_map = {'Detractor': 0, 'Passive': 1, 'Promoter':2}\n",
    "        tmp_df = all_df['NPS® Breakdown']\n",
    "\n",
    "        # Excute the label change, replacing the NPS Breakdown string with the integer\n",
    "        #tmp_df.replace({'NPS® Breakdown': label_map}, inplace=True)\n",
    "        # now create the new_df with appropriate columns\n",
    "        NPS_code = nps_to_code(all_df['NPS® Breakdown'].values.tolist())\n",
    "        \n",
    "        new_df = all_df[['Location','Workforce','NPS® Breakdown']].copy()\n",
    "        new_df['NPS_Code'] = NPS_code\n",
    "\n",
    "        # Splitting pd.Series to list to perform the sentiment analysis and\n",
    "        # text cleanup\n",
    "\n",
    "        temp = []\n",
    "        data_to_list = all_df['NPS® Comment']\n",
    "        \n",
    "        for i in range(len(data_to_list)):\n",
    "            temp.append(cleanup_data(data_to_list[i]))\n",
    "        \n",
    "        new_df['NPSCommentCleaned'] = temp\n",
    "        new_df['NPSCommentPolarity'], new_df['NPSCommentSubjectivity'] = tb_enrich(temp)\n",
    "        \n",
    "        temp = []\n",
    "        data_to_list = all_df['Overall Comment'].values.tolist()\n",
    "        for i in range(len(data_to_list)):\n",
    "            temp.append(cleanup_data(data_to_list[i]))\n",
    "        new_df['OverallCommentCleaned'] = temp\n",
    "        \n",
    "        OverallCommentPolarity, OverallCommentSubjectivity = tb_enrich(temp)\n",
    "        new_df['OverallCommentPolarity'], new_df['OverallCommentSubjectivity'] = OverallCommentPolarity, OverallCommentSubjectivity\n",
    "        \n",
    "        # write the store file\n",
    "        new_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    # now concatenate all of the resulting files into a single district .csv file\n",
    "    total_df = pd.DataFrame()\n",
    "    for infile in output_filename_list:\n",
    "        df = pd.read_csv(infile)\n",
    "        total_df = pd.concat([total_df, df], axis=0)\n",
    "        #os.remove(infile)\n",
    "\n",
    "    print(f'..Writing {cleaned_filename} with a total of {total_df.shape[0]} comments')\n",
    "    total_df.to_csv(cleaned_filename, index=False)\n",
    "    \n",
    "    write_comments(district, storelist)\n",
    "    \n",
    "    print('Done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing market: 21\n",
      "..Processing District: 125\n",
      "...Processing Store: 00154\n",
      "...Processing Store: 00161\n",
      "...Processing Store: 00266\n",
      "...Processing Store: 00274\n",
      "...Processing Store: 00617\n",
      "...Processing Store: 00494\n",
      "...Processing Store: 00790\n",
      "...Processing Store: 01252\n",
      "...Processing Store: 01474\n",
      "..Writing ../data/clean/NPS_District_125.csv with a total of 1271 comments\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# We are in Market 21, District 125\n",
    "NPS_cleanup(21, 125)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_m1new)",
   "language": "python",
   "name": "ai_m1new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
