{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "This section is where we prepare for the project, through a variety of initial steps. The steps in this section are as follows:\n",
    "\n",
    "- Importing Packages\n",
    "- Importing Data\n",
    "- Dropping NA Values\n",
    "- Subsetting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "#from pandas_profiling import ProfileReport\n",
    "\n",
    "#TextBlob Featuresc\n",
    "from textblob import TextBlob\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#SciKit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Tensorflow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Test\n",
    "from collections import Counter\n",
    "import bby\n",
    "import bby.util as ut\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26703, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Drop_Columns = ['Location',\t'Workforce', 'NPS® Breakdown', 'NPSCommentCleaned',\n",
    "                'NPSCommentPolarity', 'NPSCommentSubjectivity',\t'OverallCommentCleaned',\n",
    "                'OverallCommentPolarity', 'OverallCommentSubjectivity']\n",
    "Sentiment_Columns = ['NPSCommentPolarity', 'NPSCommentSubjectivity', 'OverallCommentPolarity', 'OverallCommentSubjectivity']\n",
    "\n",
    "# National NPS extract processed by CleanNPS_National.py - this cleans and lemmatises the NPS and Overall\n",
    "# comments.\n",
    "# \n",
    "\n",
    "all_path = \"../data/clean/NPS_NATL_subset.csv\"\n",
    "raw_df = pd.read_csv(all_path)\n",
    "\n",
    "\n",
    "# Drop everything but what we're going to use\n",
    "all_df = raw_df.drop(['Location',\t'Workforce', 'NPS® Breakdown',\n",
    "'NPSCommentPolarity', 'NPSCommentSubjectivity',\t'OverallCommentCleaned',\n",
    "'OverallCommentPolarity', 'OverallCommentSubjectivity'], axis=1)\n",
    "\n",
    "print(all_df.shape)\n",
    "# print(all_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Visualisation\n",
    "\n",
    "We can display basic statistics about the data using pandas, and also view a few entries of the dataset, to see example points with which we'll work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26703, 5)\n"
     ]
    }
   ],
   "source": [
    "#Convert the \"NPS® Breakdown\" column into indexes\n",
    "all_df[\"Sentiment\"] = all_df['NPS_Code'].copy()\n",
    "all_df = all_df.drop(['NPS_Code'], axis=1)\n",
    "all_df.head()\n",
    "print(all_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "We create tokens for the most common words in the dataset, so we can represent the presence of words in our created corpus (the n most common words) with a list of integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['staff in store in person close by when need them', 'adieb anbari was beyond helpful he answered all my questions got me in and out of there as fast as possible and even entertained my year old by answering all her questions and making boat out of paper for her', 'quick and knowledgeable', 'he called back quickly within minutes and was very good at explaining the reason for our issue', 'had really good experience thanks to your tech named ricky']\n"
     ]
    }
   ],
   "source": [
    "def str_it(_ls):\n",
    "    ls = str(_ls)\n",
    "    word_tokens = word_tokenize(ls)\n",
    "    ls = [w for w in word_tokens]\n",
    "\n",
    "    ls = \" \".join(ls)\n",
    "    return ls\n",
    "    \n",
    "\n",
    "#Define the Tokeniser\n",
    "tokeniser = Tokenizer()\n",
    "nps_list = all_df['NPSCommentCleaned'].apply(str_it)\n",
    "data_words = list(ut.sent_to_words(nps_list))\n",
    "\n",
    "\n",
    "data = []\n",
    "for i in range(len(data_words)):\n",
    "    data.append(ut.detokenize(data_words[i]))\n",
    "print(data[:5])\n",
    "\n",
    "#Create the corpus by finding the most common words\n",
    "tokeniser.fit_on_texts(data)\n",
    "\n",
    "#Tokenise our column lemmatised NPS comments\n",
    "nps_tokens = tokeniser.texts_to_matrix(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# initialize\n",
    "#cv = CountVectorizer(stop_words='english') \n",
    "#cv_matrix = cv.fit_transform([Vocab_str])\n",
    "# create document term matrix\n",
    "\n",
    "#df_dtm = pd.DataFrame(cv_matrix.toarray(), index=all_df['NPSCommentCleaned'].values, columns=cv.get_feature_names())\n",
    "#print(df_dtm.shape)\n",
    "#df_dtm.head()\n",
    "\n",
    "#print(\"The total wordcount dict: \", tokeniser.document_count)\n",
    "#print(Vocab_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Tokenised Strings to the DataFrame\n",
    "\n",
    "Currently, the tokens are contained in a matrix titled \"nps_tokens\". We then want to combine these back into the dataframe containing all of the current data. This is completed below, and then we test to make sure that this has occurred correctly by looking at the number of columns compared to that of the original matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'nps_freqs' from 'bby.util' (/Users/suchanek/repos/npsML/bby/bby/util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/suchanek/repos/npsML/notebooks/NPS analysis bag of words2.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000011?line=0'>1</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000011?line=1'>2</a>\u001b[0m \u001b[39m# New stuff\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000011?line=2'>3</a>\u001b[0m \u001b[39m# \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000011?line=6'>7</a>\u001b[0m \u001b[39m# user Counter to count the words in our NPS data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000011?line=7'>8</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000011?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m nps_freqs\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000011?line=11'>12</a>\u001b[0m vocab \u001b[39m=\u001b[39m Counter()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000011?line=12'>13</a>\u001b[0m Topwords \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'nps_freqs' from 'bby.util' (/Users/suchanek/repos/npsML/bby/bby/util.py)"
     ]
    }
   ],
   "source": [
    "#\n",
    "# New stuff\n",
    "# \n",
    "# print(all_df.shape)\n",
    "# print(nps_tokens.shape)\n",
    "\n",
    "# user Counter to count the words in our NPS data\n",
    "#\n",
    "\n",
    "from bby.util import nps_freqs\n",
    "\n",
    "vocab = Counter()\n",
    "Topwords = dict()\n",
    "\n",
    "nps_stringlist = raw_df['NPSCommentCleaned'].values.tolist()\n",
    "\n",
    "Freqs = nps_freqs(nps_stringlist, 30)\n",
    "Freqs.plot(30)\n",
    "Freqs.tabulate(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('staff', 780),\n",
       "             ('in', 6886),\n",
       "             ('store', 2520),\n",
       "             ('person', 991),\n",
       "             ('close', 65),\n",
       "             ('by', 1019),\n",
       "             ('when', 3244),\n",
       "             ('need', 688),\n",
       "             ('them', 1289),\n",
       "             ('adieb', 5),\n",
       "             ('anbari', 1),\n",
       "             ('was', 18180),\n",
       "             ('beyond', 159),\n",
       "             ('helpful', 2459),\n",
       "             ('he', 2831),\n",
       "             ('answered', 254),\n",
       "             ('all', 2080),\n",
       "             ('my', 14810),\n",
       "             ('questions', 576),\n",
       "             ('got', 1492),\n",
       "             ('me', 6839),\n",
       "             ('and', 20004),\n",
       "             ('out', 1658),\n",
       "             ('of', 6073),\n",
       "             ('there', 1830),\n",
       "             ('as', 1780),\n",
       "             ('fast', 525),\n",
       "             ('possible', 90),\n",
       "             ('even', 1047),\n",
       "             ('entertained', 1),\n",
       "             ('year', 223),\n",
       "             ('old', 754),\n",
       "             ('answering', 57),\n",
       "             ('her', 271),\n",
       "             ('making', 171),\n",
       "             ('boat', 2),\n",
       "             ('paper', 54),\n",
       "             ('for', 6334),\n",
       "             ('quick', 531),\n",
       "             ('knowledgeable', 1333),\n",
       "             ('called', 849),\n",
       "             ('back', 2649),\n",
       "             ('quickly', 370),\n",
       "             ('within', 164),\n",
       "             ('minutes', 725),\n",
       "             ('very', 4885),\n",
       "             ('good', 2308),\n",
       "             ('at', 3016),\n",
       "             ('explaining', 108),\n",
       "             ('the', 28688),\n",
       "             ('reason', 209),\n",
       "             ('our', 736),\n",
       "             ('issue', 1369),\n",
       "             ('had', 5210),\n",
       "             ('really', 531),\n",
       "             ('experience', 1110),\n",
       "             ('thanks', 104),\n",
       "             ('to', 24206),\n",
       "             ('your', 983),\n",
       "             ('tech', 1398),\n",
       "             ('named', 29),\n",
       "             ('ricky', 3),\n",
       "             ('because', 2538),\n",
       "             ('geek', 3411),\n",
       "             ('squad', 3295),\n",
       "             ('is', 3754),\n",
       "             ('best', 1864),\n",
       "             ('eli', 3),\n",
       "             ('extremely', 469),\n",
       "             ('did', 3317),\n",
       "             ('what', 2082),\n",
       "             ('needed', 1044),\n",
       "             ('help', 1709),\n",
       "             ('great', 2193),\n",
       "             ('waiting', 404),\n",
       "             ('line', 396),\n",
       "             ('with', 5765),\n",
       "             ('appointment', 2358),\n",
       "             ('not', 8163),\n",
       "             ('pleasant', 186),\n",
       "             ('service', 6507),\n",
       "             ('superb', 27),\n",
       "             ('that', 6633),\n",
       "             ('man', 251),\n",
       "             ('rami', 1),\n",
       "             ('packing', 3),\n",
       "             ('some', 686),\n",
       "             ('serious', 19),\n",
       "             ('shawarma', 1),\n",
       "             ('would', 2536),\n",
       "             ('come', 590),\n",
       "             ('just', 1267),\n",
       "             ('him', 451),\n",
       "             ('uwu', 1),\n",
       "             ('they', 8574),\n",
       "             ('worked', 470),\n",
       "             ('easy', 307),\n",
       "             ('darren', 1),\n",
       "             ('job', 1109),\n",
       "             ('restarting', 3),\n",
       "             ('cell', 80),\n",
       "             ('phone', 2414),\n",
       "             ('it', 10417),\n",
       "             ('done', 1475),\n",
       "             ('scary', 4),\n",
       "             ('but', 4017),\n",
       "             ('thankful', 14),\n",
       "             ('techs', 261),\n",
       "             ('were', 3478),\n",
       "             ('informative', 129),\n",
       "             ('concerned', 42),\n",
       "             ('welcoming', 11),\n",
       "             ('work', 1878),\n",
       "             ('on', 4801),\n",
       "             ('problem', 2720),\n",
       "             ('have', 4840),\n",
       "             ('communication', 398),\n",
       "             ('jose', 12),\n",
       "             ('exceptional', 67),\n",
       "             ('first', 766),\n",
       "             ('installation', 394),\n",
       "             ('from', 1884),\n",
       "             ('happy', 494),\n",
       "             ('excellent', 849),\n",
       "             ('explained', 445),\n",
       "             ('detailed', 23),\n",
       "             ('how', 804),\n",
       "             ('backup', 118),\n",
       "             ('camera', 226),\n",
       "             ('extended', 39),\n",
       "             ('if', 1303),\n",
       "             ('ever', 296),\n",
       "             ('any', 619),\n",
       "             ('please', 120),\n",
       "             ('contact', 250),\n",
       "             ('immediately', 106),\n",
       "             ('awesome', 175),\n",
       "             ('thank', 178),\n",
       "             ('you', 1914),\n",
       "             ('buy', 1758),\n",
       "             ('associates', 64),\n",
       "             ('friendly', 1132),\n",
       "             ('no', 2125),\n",
       "             ('one', 1806),\n",
       "             ('else', 222),\n",
       "             ('neighborhood', 5),\n",
       "             ('porque', 7),\n",
       "             ('resolvieron', 2),\n",
       "             ('mi', 11),\n",
       "             ('problema', 3),\n",
       "             ('bien', 3),\n",
       "             ('rapido', 1),\n",
       "             ('gentleman', 129),\n",
       "             ('spent', 245),\n",
       "             ('much', 480),\n",
       "             ('time', 3091),\n",
       "             ('efficiently', 48),\n",
       "             ('while', 490),\n",
       "             ('being', 494),\n",
       "             ('upbeat', 3),\n",
       "             ('ask', 265),\n",
       "             ('everything', 742),\n",
       "             ('promised', 153),\n",
       "             ('less', 207),\n",
       "             ('than', 897),\n",
       "             ('original', 151),\n",
       "             ('estimate', 60),\n",
       "             ('employees', 276),\n",
       "             ('polite', 349),\n",
       "             ('attentive', 79),\n",
       "             ('ready', 508),\n",
       "             ('though', 379),\n",
       "             ('didnt', 1132),\n",
       "             ('make', 862),\n",
       "             ('an', 2954),\n",
       "             ('could', 1736),\n",
       "             ('been', 1312),\n",
       "             ('better', 299),\n",
       "             ('satisfied', 296),\n",
       "             ('luis', 10),\n",
       "             ('young', 247),\n",
       "             ('lady', 159),\n",
       "             ('helped', 820),\n",
       "             ('both', 294),\n",
       "             ('interested', 58),\n",
       "             ('courteously', 8),\n",
       "             ('fixed', 1663),\n",
       "             ('alex', 56),\n",
       "             ('exemplary', 6),\n",
       "             ('employee', 285),\n",
       "             ('who', 1068),\n",
       "             ('professional', 916),\n",
       "             ('during', 174),\n",
       "             ('scheduled', 284),\n",
       "             ('meeting', 24),\n",
       "             ('finished', 102),\n",
       "             ('pleased', 164),\n",
       "             ('task', 67),\n",
       "             ('super', 165),\n",
       "             ('understanding', 121),\n",
       "             ('pc', 357),\n",
       "             ('everyone', 208),\n",
       "             ('deal', 103),\n",
       "             ('definitely', 81),\n",
       "             ('worth', 96),\n",
       "             ('protection', 133),\n",
       "             ('plan', 222),\n",
       "             ('jesse', 11),\n",
       "             ('customer', 1414),\n",
       "             ('point', 127),\n",
       "             ('convenient', 69),\n",
       "             ('complaints', 21),\n",
       "             ('here', 95),\n",
       "             ('chris', 41),\n",
       "             ('took', 2001),\n",
       "             ('be', 2962),\n",
       "             ('are', 1381),\n",
       "             ('more', 878),\n",
       "             ('chat', 88),\n",
       "             ('people', 753),\n",
       "             ('purchasing', 57),\n",
       "             ('recommendations', 53),\n",
       "             ('most', 266),\n",
       "             ('known', 53),\n",
       "             ('software', 234),\n",
       "             ('fixes', 17),\n",
       "             ('johnny', 4),\n",
       "             ('response', 275),\n",
       "             ('repaired', 380),\n",
       "             ('returned', 382),\n",
       "             ('thorough', 123),\n",
       "             ('seem', 167),\n",
       "             ('condescending', 18),\n",
       "             ('recieved', 8),\n",
       "             ('miller', 2),\n",
       "             ('understand', 321),\n",
       "             ('answer', 234),\n",
       "             ('clayton', 3),\n",
       "             ('especially', 71),\n",
       "             ('patient', 442),\n",
       "             ('once', 290),\n",
       "             ('speak', 177),\n",
       "             ('someone', 559),\n",
       "             ('she', 662),\n",
       "             ('resolved', 513),\n",
       "             ('mainly', 9),\n",
       "             ('paul', 9),\n",
       "             ('only', 1151),\n",
       "             ('isnt', 70),\n",
       "             ('wait', 869),\n",
       "             ('longer', 332),\n",
       "             ('should', 585),\n",
       "             ('other', 593),\n",
       "             ('professionalism', 53),\n",
       "             ('attentiveness', 3),\n",
       "             ('needs', 239),\n",
       "             ('geeks', 131),\n",
       "             ('knowlegeable', 7),\n",
       "             ('im', 349),\n",
       "             ('still', 1303),\n",
       "             ('trying', 351),\n",
       "             ('learn', 38),\n",
       "             ('new', 2056),\n",
       "             ('bells', 3),\n",
       "             ('whistles', 3),\n",
       "             ('honest', 79),\n",
       "             ('responsive', 78),\n",
       "             ('diligently', 5),\n",
       "             ('fix', 1383),\n",
       "             ('know', 849),\n",
       "             ('anything', 364),\n",
       "             ('about', 1246),\n",
       "             ('computers', 250),\n",
       "             ('so', 2161),\n",
       "             ('zak', 3),\n",
       "             ('completely', 211),\n",
       "             ('clear', 115),\n",
       "             ('information', 420),\n",
       "             ('computer', 4886),\n",
       "             ('after', 1549),\n",
       "             ('his', 487),\n",
       "             ('passing', 3),\n",
       "             ('team', 257),\n",
       "             ('fantastic', 95),\n",
       "             ('david', 34),\n",
       "             ('sebastian', 4),\n",
       "             ('kept', 272),\n",
       "             ('loop', 14),\n",
       "             ('every', 215),\n",
       "             ('step', 51),\n",
       "             ('way', 585),\n",
       "             ('always', 606),\n",
       "             ('turnaround', 50),\n",
       "             ('solved', 418),\n",
       "             ('went', 1005),\n",
       "             ('respectful', 59),\n",
       "             ('promptly', 51),\n",
       "             ('efficient', 413),\n",
       "             ('brandon', 38),\n",
       "             ('expected', 239),\n",
       "             ('which', 846),\n",
       "             ('next', 429),\n",
       "             ('day', 931),\n",
       "             ('get', 2803),\n",
       "             ('laptop', 2273),\n",
       "             ('courteous', 444),\n",
       "             ('found', 341),\n",
       "             ('its', 379),\n",
       "             ('little', 297),\n",
       "             ('faster', 75),\n",
       "             ('think', 308),\n",
       "             ('has', 673),\n",
       "             ('something', 311),\n",
       "             ('do', 1673),\n",
       "             ('internet', 123),\n",
       "             ('spectrum', 5),\n",
       "             ('helping', 197),\n",
       "             ('skills', 67),\n",
       "             ('effective', 59),\n",
       "             ('timely', 284),\n",
       "             ('opinion', 45),\n",
       "             ('technician', 572),\n",
       "             ('issues', 654),\n",
       "             ('emmanuel', 2),\n",
       "             ('concerns', 83),\n",
       "             ('made', 730),\n",
       "             ('suggestions', 19),\n",
       "             ('correct', 190),\n",
       "             ('sure', 380),\n",
       "             ('joe', 16),\n",
       "             ('personable', 63),\n",
       "             ('funny', 11),\n",
       "             ('smooth', 39),\n",
       "             ('also', 694),\n",
       "             ('works', 127),\n",
       "             ('now', 802),\n",
       "             ('competent', 107),\n",
       "             ('having', 567),\n",
       "             ('look', 172),\n",
       "             ('gives', 16),\n",
       "             ('peace', 9),\n",
       "             ('mind', 34),\n",
       "             ('taking', 141),\n",
       "             ('their', 666),\n",
       "             ('explain', 209),\n",
       "             ('costumer', 10),\n",
       "             ('will', 894),\n",
       "             ('refer', 24),\n",
       "             ('rob', 3),\n",
       "             ('prompt', 173),\n",
       "             ('kyle', 13),\n",
       "             ('getting', 392),\n",
       "             ('wanted', 453),\n",
       "             ('provided', 212),\n",
       "             ('individuals', 36),\n",
       "             ('care', 508),\n",
       "             ('installing', 75),\n",
       "             ('car', 386),\n",
       "             ('head', 37),\n",
       "             ('unit', 134),\n",
       "             ('problems', 608),\n",
       "             ('program', 159),\n",
       "             ('affordably', 1),\n",
       "             ('hacked', 46),\n",
       "             ('told', 2404),\n",
       "             ('john', 30),\n",
       "             ('workmanship', 9),\n",
       "             ('agent', 829),\n",
       "             ('dealt', 91),\n",
       "             ('skillfully', 1),\n",
       "             ('personal', 75),\n",
       "             ('attention', 87),\n",
       "             ('austin', 22),\n",
       "             ('puyallup', 1),\n",
       "             ('branch', 10),\n",
       "             ('ive', 187),\n",
       "             ('spoke', 192),\n",
       "             ('learning', 13),\n",
       "             ('neil', 1),\n",
       "             ('extra', 143),\n",
       "             ('mile', 27),\n",
       "             ('expertise', 54),\n",
       "             ('excellence', 7),\n",
       "             ('asked', 812),\n",
       "             ('things', 462),\n",
       "             ('address', 94),\n",
       "             ('nick', 39),\n",
       "             ('wwas', 1),\n",
       "             ('terrific', 31),\n",
       "             ('lucky', 13),\n",
       "             ('crews', 2),\n",
       "             ('right', 487),\n",
       "             ('top', 199),\n",
       "             ('diagnosed', 36),\n",
       "             ('set', 495),\n",
       "             ('up', 3281),\n",
       "             ('repair', 1313),\n",
       "             ('days', 1065),\n",
       "             ('brian', 14),\n",
       "             ('nicest', 2),\n",
       "             ('encountered', 18),\n",
       "             ('experienced', 56),\n",
       "             ('nice', 613),\n",
       "             ('side', 58),\n",
       "             ('world', 27),\n",
       "             ('stay', 37),\n",
       "             ('blessed', 6),\n",
       "             ('well', 661),\n",
       "             ('am', 843),\n",
       "             ('impressed', 66),\n",
       "             ('technical', 95),\n",
       "             ('knowledge', 253),\n",
       "             ('listen', 70),\n",
       "             ('analyze', 3),\n",
       "             ('solve', 272),\n",
       "             ('appreciated', 78),\n",
       "             ('advice', 79),\n",
       "             ('remaining', 8),\n",
       "             ('came', 569),\n",
       "             ('house', 118),\n",
       "             ('printer', 231),\n",
       "             ('this', 2178),\n",
       "             ('replacement', 324),\n",
       "             ('cannot', 190),\n",
       "             ('properly', 196),\n",
       "             ('knew', 247),\n",
       "             ('exactly', 104),\n",
       "             ('talking', 103),\n",
       "             ('pleasure', 25),\n",
       "             ('two', 731),\n",
       "             ('different', 381),\n",
       "             ('immensely', 4),\n",
       "             ('identified', 23),\n",
       "             ('matter', 54),\n",
       "             ('geeksquad', 75),\n",
       "             ('walk', 91),\n",
       "             ('through', 361),\n",
       "             ('steps', 34),\n",
       "             ('bios', 13),\n",
       "             ('reboot', 10),\n",
       "             ('upgraded', 27),\n",
       "             ('hard', 668),\n",
       "             ('drive', 871),\n",
       "             ('install', 566),\n",
       "             ('guys', 282),\n",
       "             ('showed', 210),\n",
       "             ('pick', 716),\n",
       "             ('items', 130),\n",
       "             ('kind', 340),\n",
       "             ('cranky', 1),\n",
       "             ('disposition', 3),\n",
       "             ('doing', 380),\n",
       "             ('wrong', 377),\n",
       "             ('surpassed', 2),\n",
       "             ('expectations', 83),\n",
       "             ('satisfactory', 64),\n",
       "             ('we', 1753),\n",
       "             ('treated', 113),\n",
       "             ('personnel', 83),\n",
       "             ('accepted', 21),\n",
       "             ('away', 287),\n",
       "             ('subsequent', 4),\n",
       "             ('silverdale', 5),\n",
       "             ('location', 234),\n",
       "             ('short', 130),\n",
       "             ('amount', 92),\n",
       "             ('expert', 56),\n",
       "             ('looking', 66),\n",
       "             ('returning', 57),\n",
       "             ('recommending', 19),\n",
       "             ('friends', 42),\n",
       "             ('family', 42),\n",
       "             ('blake', 6),\n",
       "             ('cared', 20),\n",
       "             ('client', 18),\n",
       "             ('leave', 191),\n",
       "             ('quality', 140),\n",
       "             ('installer', 111),\n",
       "             ('amazing', 162),\n",
       "             ('accurate', 50),\n",
       "             ('pricing', 31),\n",
       "             ('helper', 15),\n",
       "             ('instructor', 3),\n",
       "             ('seems', 153),\n",
       "             ('acknowledgeable', 3),\n",
       "             ('basic', 48),\n",
       "             ('associate', 168),\n",
       "             ('charles', 12),\n",
       "             ('accomodating', 7),\n",
       "             ('met', 84),\n",
       "             ('alfredo', 1),\n",
       "             ('absolute', 17),\n",
       "             ('actually', 189),\n",
       "             ('order', 289),\n",
       "             ('apple', 757),\n",
       "             ('function', 25),\n",
       "             ('highly', 51),\n",
       "             ('without', 506),\n",
       "             ('security', 63),\n",
       "             ('knowing', 45),\n",
       "             ('nearby', 8),\n",
       "             ('true', 60),\n",
       "             ('provide', 88),\n",
       "             ('business', 155),\n",
       "             ('genuinely', 14),\n",
       "             ('grateful', 35),\n",
       "             ('received', 682),\n",
       "             ('convenience', 16),\n",
       "             ('trustworthy', 12),\n",
       "             ('stereo', 186),\n",
       "             ('installed', 547),\n",
       "             ('lawrence', 4),\n",
       "             ('ks', 4),\n",
       "             ('mike', 38),\n",
       "             ('goetz', 1),\n",
       "             ('another', 819),\n",
       "             ('guy', 410),\n",
       "             ('obviously', 23),\n",
       "             ('seemed', 228),\n",
       "             ('eager', 11),\n",
       "             ('turns', 28),\n",
       "             ('nothing', 484),\n",
       "             ('unrelated', 7),\n",
       "             ('yet', 209),\n",
       "             ('responsibility', 22),\n",
       "             ('find', 512),\n",
       "             ('kudos', 7),\n",
       "             ('manager', 345),\n",
       "             ('vehicle', 120),\n",
       "             ('electronics', 36),\n",
       "             ('department', 55),\n",
       "             ('hired', 8),\n",
       "             ('never', 806),\n",
       "             ('company', 143),\n",
       "             ('before', 796),\n",
       "             ('surprise', 9),\n",
       "             ('arizona', 3),\n",
       "             ('total', 282),\n",
       "             ('level', 98),\n",
       "             ('trust', 61),\n",
       "             ('these', 141),\n",
       "             ('extraordinary', 6),\n",
       "             ('marc', 4),\n",
       "             ('imac', 36),\n",
       "             ('daddy', 2),\n",
       "             ('assistance', 142),\n",
       "             ('resolving', 52),\n",
       "             ('stayed', 26),\n",
       "             ('promise', 22),\n",
       "             ('battery', 614),\n",
       "             ('almost', 220),\n",
       "             ('week', 385),\n",
       "             ('early', 102),\n",
       "             ('online', 357),\n",
       "             ('brandan', 1),\n",
       "             ('exceptionally', 15),\n",
       "             ('management', 55),\n",
       "             ('above', 137),\n",
       "             ('lack', 171),\n",
       "             ('technology', 43),\n",
       "             ('certified', 18),\n",
       "             ('assist', 73),\n",
       "             ('kays', 1),\n",
       "             ('upgrade', 56),\n",
       "             ('drone', 3),\n",
       "             ('williams', 2),\n",
       "             ('treat', 19),\n",
       "             ('fairly', 37),\n",
       "             ('age', 19),\n",
       "             ('discrimation', 1),\n",
       "             ('reps', 62),\n",
       "             ('plesent', 1),\n",
       "             ('perfect', 142),\n",
       "             ('included', 38),\n",
       "             ('device', 534),\n",
       "             ('listened', 75),\n",
       "             ('addressed', 84),\n",
       "             ('enjoyable', 4),\n",
       "             ('checking', 73),\n",
       "             ('discussing', 5),\n",
       "             ('gave', 436),\n",
       "             ('knowledgable', 81),\n",
       "             ('alto', 1),\n",
       "             ('ways', 24),\n",
       "             ('comes', 57),\n",
       "             ('mess', 37),\n",
       "             ('visit', 286),\n",
       "             ('caring', 27),\n",
       "             ('bridgette', 1),\n",
       "             ('bridget', 2),\n",
       "             ('support', 383),\n",
       "             ('gonna', 13),\n",
       "             ('professionally', 30),\n",
       "             ('mark', 20),\n",
       "             ('taken', 263),\n",
       "             ('due', 254),\n",
       "             ('programs', 136),\n",
       "             ('checked', 250),\n",
       "             ('couple', 142),\n",
       "             ('websites', 4),\n",
       "             ('ones', 43),\n",
       "             ('using', 153),\n",
       "             ('representative', 210),\n",
       "             ('home', 829),\n",
       "             ('us', 552),\n",
       "             ('speed', 46),\n",
       "             ('goal', 7),\n",
       "             ('succesful', 1),\n",
       "             ('jessica', 4),\n",
       "             ('such', 105),\n",
       "             ('updated', 97),\n",
       "             ('recommend', 294),\n",
       "             ('anyone', 208),\n",
       "             ('until', 366),\n",
       "             ('later', 457),\n",
       "             ('earlier', 67),\n",
       "             ('appointments', 240),\n",
       "             ('live', 96),\n",
       "             ('idaho', 1),\n",
       "             ('falls', 7),\n",
       "             ('into', 441),\n",
       "             ('schedule', 218),\n",
       "             ('ability', 42),\n",
       "             ('see', 350),\n",
       "             ('accommodating', 37),\n",
       "             ('used', 257),\n",
       "             ('repairs', 262),\n",
       "             ('along', 60),\n",
       "             ('many', 266),\n",
       "             ('years', 265),\n",
       "             ('disappointed', 261),\n",
       "             ('explains', 5),\n",
       "             ('offers', 9),\n",
       "             ('answers', 107),\n",
       "             ('miracle', 4),\n",
       "             ('worker', 34),\n",
       "             ('current', 35),\n",
       "             ('glitch', 4),\n",
       "             ('corrected', 79),\n",
       "             ('recommended', 99),\n",
       "             ('friend', 43),\n",
       "             ('same', 609),\n",
       "             ('outstanding', 103),\n",
       "             ('thru', 37),\n",
       "             ('process', 443),\n",
       "             ('windows', 223),\n",
       "             ('able', 913),\n",
       "             ('open', 162),\n",
       "             ('regularly', 2),\n",
       "             ('plus', 123),\n",
       "             ('tablet', 108),\n",
       "             ('understood', 86),\n",
       "             ('services', 211),\n",
       "             ('brought', 539),\n",
       "             ('list', 38),\n",
       "             ('iphone', 297),\n",
       "             ('yasmine', 1),\n",
       "             ('machine', 89),\n",
       "             ('least', 141),\n",
       "             ('potential', 5),\n",
       "             ('roi', 1),\n",
       "             ('buying', 61),\n",
       "             ('additional', 137),\n",
       "             ('stull', 1),\n",
       "             ('let', 185),\n",
       "             ('decide', 8),\n",
       "             ('check', 332),\n",
       "             ('upon', 107),\n",
       "             ('greeted', 26),\n",
       "             ('cameron', 11),\n",
       "             ('mario', 1),\n",
       "             ('felt', 218),\n",
       "             ('comfortable', 41),\n",
       "             ('leaving', 75),\n",
       "             ('hands', 23),\n",
       "             ('couldnt', 383),\n",
       "             ('own', 128),\n",
       "             ('source', 18),\n",
       "             ('thankyou', 1),\n",
       "             ('whole', 161),\n",
       "             ('damon', 4),\n",
       "             ('tv', 111),\n",
       "             ('section', 9),\n",
       "             ('receive', 154),\n",
       "             ('acknowledgement', 3),\n",
       "             ('primary', 15),\n",
       "             ('rep', 261),\n",
       "             ('transfer', 379),\n",
       "             ('data', 610),\n",
       "             ('accomplished', 42),\n",
       "             ('theyre', 45),\n",
       "             ('troubleshoot', 18),\n",
       "             ('representing', 1),\n",
       "             ('bestbuy', 88),\n",
       "             ('courteoushad', 1),\n",
       "             ('manner', 177),\n",
       "             ('nobody', 80),\n",
       "             ('lol', 9),\n",
       "             ('products', 108),\n",
       "             ('left', 499),\n",
       "             ('going', 499),\n",
       "             ('demeanor', 7),\n",
       "             ('hunter', 4),\n",
       "             ('tommy', 6),\n",
       "             ('handled', 76),\n",
       "             ('kenny', 2),\n",
       "             ('frustrations', 5),\n",
       "             ('slow', 241),\n",
       "             ('thoroughly', 33),\n",
       "             ('meet', 41),\n",
       "             ('empathetic', 5),\n",
       "             ('considerate', 19),\n",
       "             ('janus', 1),\n",
       "             ('bethel', 1),\n",
       "             ('park', 15),\n",
       "             ('ipad', 250),\n",
       "             ('absolutely', 125),\n",
       "             ('incredible', 13),\n",
       "             ('blocked', 7),\n",
       "             ('helpfull', 10),\n",
       "             ('wonderfully', 3),\n",
       "             ('actively', 2),\n",
       "             ('testing', 19),\n",
       "             ('sales', 98),\n",
       "             ('assisted', 77),\n",
       "             ('lab', 9),\n",
       "             ('pollock', 1),\n",
       "             ('behind', 72),\n",
       "             ('counter', 145),\n",
       "             ('running', 136),\n",
       "             ('far', 143),\n",
       "             ('belief', 2),\n",
       "             ('picked', 417),\n",
       "             ('want', 294),\n",
       "             ('happened', 148),\n",
       "             ('whenever', 18),\n",
       "             ('installers', 40),\n",
       "             ('assessed', 8),\n",
       "             ('advised', 74),\n",
       "             ('offered', 123),\n",
       "             ('reasonable', 70),\n",
       "             ('solutions', 44),\n",
       "             ('said', 1453),\n",
       "             ('use', 519),\n",
       "             ('follow', 107),\n",
       "             ('ups', 28),\n",
       "             ('withgeek', 1),\n",
       "             ('expectation', 14),\n",
       "             ('familiar', 18),\n",
       "             ('product', 301),\n",
       "             ('recently', 58),\n",
       "             ('prior', 111),\n",
       "             ('turn', 201),\n",
       "             ('around', 260),\n",
       "             ('aj', 12),\n",
       "             ('geoff', 1),\n",
       "             ('member', 237),\n",
       "             ('wish', 93),\n",
       "             ('spend', 80),\n",
       "             ('jennifer', 3),\n",
       "             ('fixing', 112),\n",
       "             ('ahead', 63),\n",
       "             ('reminded', 10),\n",
       "             ('ifwhen', 1),\n",
       "             ('sell', 57),\n",
       "             ('take', 877),\n",
       "             ('put', 392),\n",
       "             ('dash', 85),\n",
       "             ('like', 839),\n",
       "             ('human', 18),\n",
       "             ('detail', 51),\n",
       "             ('wonderful', 126),\n",
       "             ('keeping', 32),\n",
       "             ('chose', 73),\n",
       "             ('squads', 28),\n",
       "             ('competence', 7),\n",
       "             ('courtesy', 31),\n",
       "             ('chrome', 52),\n",
       "             ('book', 57),\n",
       "             ('timeframe', 14),\n",
       "             ('dakota', 2),\n",
       "             ('walked', 105),\n",
       "             ('confident', 35),\n",
       "             ('greatly', 16),\n",
       "             ('reservation', 40),\n",
       "             ('waited', 337),\n",
       "             ('fromstart', 1),\n",
       "             ('previous', 94),\n",
       "             ('attempts', 42),\n",
       "             ('initiate', 2),\n",
       "             ('het', 1),\n",
       "             ('verytime', 1),\n",
       "             ('consuming', 21),\n",
       "             ('proised', 1),\n",
       "             ('call', 1211),\n",
       "             ('benjamin', 2),\n",
       "             ('working', 758),\n",
       "             ('perfectly', 32),\n",
       "             ('autotech', 2),\n",
       "             ('foose', 1),\n",
       "             ('father', 6),\n",
       "             ('chip', 10),\n",
       "             ('disagrees', 1),\n",
       "             ('shame', 17),\n",
       "             ('refuses', 6),\n",
       "             ('accept', 27),\n",
       "             ('peaked', 1),\n",
       "             ('authotech', 1),\n",
       "             ('setup', 96),\n",
       "             ('greek', 32),\n",
       "             ('lynchburg', 1),\n",
       "             ('energy', 5),\n",
       "             ('bcs', 2),\n",
       "             ('aaron', 6),\n",
       "             ('representatives', 38),\n",
       "             ('necessary', 62),\n",
       "             ('operational', 7),\n",
       "             ('details', 36),\n",
       "             ('fluid', 2),\n",
       "             ('language', 26),\n",
       "             ('teaching', 5),\n",
       "             ('functions', 23),\n",
       "             ('fashion', 20),\n",
       "             ('antony', 1),\n",
       "             ('solving', 41),\n",
       "             ('totally', 126),\n",
       "             ('again', 743),\n",
       "             ('patiently', 19),\n",
       "             ('merchandise', 4),\n",
       "             ('name', 156),\n",
       "             ('sandheep', 1),\n",
       "             ('or', 1332),\n",
       "             ('similar', 16),\n",
       "             ('satisified', 4),\n",
       "             ('nerd', 4),\n",
       "             ('love', 62),\n",
       "             ('three', 282),\n",
       "             ('knowlegable', 11),\n",
       "             ('gone', 122),\n",
       "             ('agreed', 62),\n",
       "             ('few', 268),\n",
       "             ('closing', 38),\n",
       "             ('springfield', 6),\n",
       "             ('james', 26),\n",
       "             ('clements', 1),\n",
       "             ('truly', 41),\n",
       "             ('feel', 344),\n",
       "             ('pressured', 4),\n",
       "             ('options', 81),\n",
       "             ('dedicated', 13),\n",
       "             ('solution', 97),\n",
       "             ('staples', 2),\n",
       "             ('bridgewater', 2),\n",
       "             ('mall', 17),\n",
       "             ('abby', 3),\n",
       "             ('affection', 1),\n",
       "             ('ester', 1),\n",
       "             ('positive', 75),\n",
       "             ('salesperson', 31),\n",
       "             ('try', 225),\n",
       "             ('bunch', 22),\n",
       "             ('parts', 321),\n",
       "             ('marlon', 1),\n",
       "             ('dominic', 5),\n",
       "             ('eric', 12),\n",
       "             ('over', 872),\n",
       "             ('serinity', 1),\n",
       "             ('spring', 3),\n",
       "             ('hill', 6),\n",
       "             ('everytime', 18),\n",
       "             ('believed', 7),\n",
       "             ('past', 179),\n",
       "             ('completed', 394),\n",
       "             ('analysis', 7),\n",
       "             ('requested', 167),\n",
       "             ('dylan', 6),\n",
       "             ('jordan', 11),\n",
       "             ('cumming', 1),\n",
       "             ('ga', 6),\n",
       "             ('matt', 29),\n",
       "             ('peters', 2),\n",
       "             ('experiences', 79),\n",
       "             ('promptness', 10),\n",
       "             ('behavior', 8),\n",
       "             ('clock', 5),\n",
       "             ('vechicle', 3),\n",
       "             ('bonus', 3),\n",
       "             ('wiped', 61),\n",
       "             ('down', 246),\n",
       "             ('inside', 59),\n",
       "             ('panels', 2),\n",
       "             ('desk', 117),\n",
       "             ('credit', 79),\n",
       "             ('tried', 440),\n",
       "             ('sending', 59),\n",
       "             ('manufacturer', 41),\n",
       "             ('loaner', 12),\n",
       "             ('mine', 53),\n",
       "             ('fine', 218),\n",
       "             ('interaction', 52),\n",
       "             ('servicing', 27),\n",
       "             ('account', 123),\n",
       "             ('giving', 63),\n",
       "             ('loved', 19),\n",
       "             ('believe', 116),\n",
       "             ('drives', 26),\n",
       "             ('white', 17),\n",
       "             ('truck', 42),\n",
       "             ('anyway', 38),\n",
       "             ('pressure', 10),\n",
       "             ('allotted', 9),\n",
       "             ('frame', 45),\n",
       "             ('sean', 16),\n",
       "             ('results', 155),\n",
       "             ('omar', 9),\n",
       "             ('entertaining', 2),\n",
       "             ('wont', 78),\n",
       "             ('waste', 182),\n",
       "             ('stupid', 49),\n",
       "             ('situation', 131),\n",
       "             ('fun', 10),\n",
       "             ('exceeded', 24),\n",
       "             ('notch', 12),\n",
       "             ('travis', 8),\n",
       "             ('informed', 223),\n",
       "             ('progress', 44),\n",
       "             ('eases', 1),\n",
       "             ('angst', 2),\n",
       "             ('choice', 52),\n",
       "             ('brcause', 1),\n",
       "             ('painless', 11),\n",
       "             ('helpfulness', 11),\n",
       "             ('talk', 258),\n",
       "             ('seen', 69),\n",
       "             ('technicians', 150),\n",
       "             ('thought', 255),\n",
       "             ('men', 29),\n",
       "             ('course', 36),\n",
       "             ('valuable', 16),\n",
       "             ('changing', 22),\n",
       "             ('screen', 665),\n",
       "             ('protector', 134),\n",
       "             ('too', 563),\n",
       "             ('available', 206),\n",
       "             ('knowledgeble', 3),\n",
       "             ('pretty', 114),\n",
       "             ('figure', 130),\n",
       "             ('entire', 134),\n",
       "             ('serve', 12),\n",
       "             ('keith', 5),\n",
       "             ('daniel', 22),\n",
       "             ('assisting', 24),\n",
       "             ('matthew', 14),\n",
       "             ('laptops', 74),\n",
       "             ('access', 132),\n",
       "             ('arise', 2),\n",
       "             ('organized', 18),\n",
       "             ('promises', 12),\n",
       "             ('wifi', 91),\n",
       "             ('driver', 25),\n",
       "             ('reinstalled', 58),\n",
       "             ('isiah', 1),\n",
       "             ('interface', 7),\n",
       "             ('courteos', 1),\n",
       "             ('phoned', 10),\n",
       "             ('donewhen', 1),\n",
       "             ('communications', 29),\n",
       "             ('agents', 228),\n",
       "             ('likely', 64),\n",
       "             ('francisco', 4),\n",
       "             ('hartsdale', 1),\n",
       "             ('ny', 11),\n",
       "             ('re', 26),\n",
       "             ('suffice', 1),\n",
       "             ('say', 184),\n",
       "             ('confirms', 2),\n",
       "             ('customers', 246),\n",
       "             ('life', 49),\n",
       "             ('satisfaction', 62),\n",
       "             ('justin', 14),\n",
       "             ('ios', 8),\n",
       "             ('appeared', 39),\n",
       "             ('comment', 24),\n",
       "             ('yes', 88),\n",
       "             ('hp', 123),\n",
       "             ('purchase', 288),\n",
       "             ('doug', 7),\n",
       "             ('managed', 16),\n",
       "             ('squeeze', 2),\n",
       "             ('loren', 3),\n",
       "             ('communicate', 61),\n",
       "             ('unwashed', 1),\n",
       "             ('tecnical', 1),\n",
       "             ('asceticism', 1),\n",
       "             ('authorized', 33),\n",
       "             ('center', 206),\n",
       "             ('handle', 53),\n",
       "             ('garage', 9),\n",
       "             ('seamless', 8),\n",
       "             ('mccabe', 1),\n",
       "             ...])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.word_counts\n",
    "#print(tokeniser.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26703, 13563)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combining the dataframe with the tokens using pd.concat\n",
    "all_df = pd.concat([all_df, pd.DataFrame(nps_tokens)], sort=False, axis=1)\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Data Preparation\n",
    "\n",
    "The data is now almost ready for a model to be trained on it, but a few final preparations will need to occur. For example, we need to drop the columns that we don't plan to use, such as the \"Tweet_Content\" column, which has had its useful information extracted already.\n",
    "\n",
    "We also split the data into a training and test set, such that we can evaluate our model's performance without touching the held-out data. We do this because if we continually test against this held-out data, it loses its usefulness as unseen \"real-world\" data.\n",
    "\n",
    "Sections under this header include:\n",
    "- Dropping Unused Data\n",
    "- Test-Train Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Unused Data\n",
    "\n",
    "We drop non-useful columns from the DataFrame here. These either have no use (Tweet ID), or have already had the useful information extracted (Tweet Content). We also remove the \"y\" or dependent variable here, so we don't accidentally train on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26703, 13561)\n"
     ]
    }
   ],
   "source": [
    "# Remove dependent variable\n",
    "# Location\tWorkforce\tNPS® Breakdown\tNPS_Code\tNPSCommentCleaned\tNPSCommentLemmatised\tNPSCommentPolarity\tNPSCommentSubjectivity\tOverallCommentCleaned\tOverallCommentLemmatised\tOverallCommentPolarity\tOverallCommentSubjectivity\tSentiment\n",
    "y = all_df[\"Sentiment\"]\n",
    "\n",
    "all_df = all_df.drop(columns=['NPSCommentCleaned', 'Sentiment'])\n",
    "\n",
    "all_df.head()\n",
    "print(all_df.shape)\n",
    "input_size = all_df.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Train Split\n",
    "\n",
    "Here, we use SciKit-Learn's inbuilt function to split our data into a test set and a train set, with the appropriate labels. We use a constant random state to make this replicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_df, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction and Training\n",
    "\n",
    "Finally, it is time to construct our model. In this case, we use a neural network constructed with Keras. We then train it with our data in the training dataset, and validate using the test datasets.\n",
    "\n",
    "Sections under this header include:\n",
    "- Model Construction\n",
    "- Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction\n",
    "\n",
    "Here, we define the neural network that we will train to predict the output. This model is constructed with the following layers:\n",
    "1. Dense\n",
    "2. Dropout\n",
    "\n",
    "The Dense layers are fully-connected layers. This means that inbetween each layer, we can transfer data from any neuron to any one in the next layer (or indeed all others), scaled by the weight associated with that transfer. These weights are trained.\n",
    "\n",
    "The Dropout layers prevent our overall weights from getting too large, as can happen with larger neural networks. This helps to stop certain areas of the network from overloading the network as a whole.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 20:15:04.893091: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-06 20:15:04.893281: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#Test model\n",
    "import datetime\n",
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, input_dim=input_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "     loss='sparse_categorical_crossentropy',\n",
    "     optimizer='adam',\n",
    "     metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# [print(i.shape, i.dtype) for i in model.inputs]\n",
    "# [print(o.shape, o.dtype) for o in model.outputs]\n",
    "# [print(l.name, l.input_shape, l.dtype) for l in model.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Next, we fit this model with our data, using backpropagation, for 30 epochs. We can view the increase in accuracy of the model through the different epochs, on both the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Before even looking at my device or seeing what was wrong with it, the employee started listing off prices.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/suchanek/repos/npsML/notebooks/NPS analysis bag of words2.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000023?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelCheckpoint\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000023?line=1'>2</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\u001b[39m\"\u001b[39m\u001b[39mbest_model_bow.hdf5\u001b[39m\u001b[39m\"\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, save_freq\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,save_weights_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000023?line=3'>4</a>\u001b[0m X_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(X_train)\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000023?line=4'>5</a>\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(y_train)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPS%20analysis%20bag%20of%20words2.ipynb#ch0000023?line=5'>6</a>\u001b[0m X_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(X_test)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Before even looking at my device or seeing what was wrong with it, the employee started listing off prices.'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"best_model_bow.hdf5\", monitor='accuracy', verbose=0, save_best_only=True, mode='auto', save_freq=1,save_weights_only=False)\n",
    "\n",
    "X_train = np.asarray(X_train).astype('float32')\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "X_test = np.asarray(X_test).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32')\n",
    "\n",
    "h = model.fit(\n",
    "     X_train, y_train,\n",
    "     validation_data=(X_test, y_test),\n",
    "     epochs=50,\n",
    "     callbacks=[tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5), tensorboard_callback, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now that we've trained the model, we can view it's accuracy with a confusion matrix. This allows us to see the predictions for Tweets with various true values. From this, we might see that we are better at predicting certain classes than others, such as in this model, where we can predict Negative and Positive sentiment significantly better than Irrelevant or Neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "#Assign labels to predictions and test data\n",
    "y_pred_labels = ut.ids_to_names(y_pred)\n",
    "y_test_labels = ut.ids_to_names(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique = list(set(y_test_labels))\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels, labels = y_unique, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y_unique)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Full Dataset\n",
    "\n",
    "Now that we are happy with our model, we can train using the full dataset, and predict the held-out test data. This involves performing all of our transformation steps on both this training dataset and the held-out test data. Luckily, we can reuse the code from above to achieve this, so little further explanation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the full dataset!\n",
    "# df = train_df\n",
    "\n",
    "# the test dataframe was loaded earlier and is named test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encode using Pandas' get_dummies()\n",
    "\n",
    "##Train\n",
    "onehot = pd.get_dummies(all_df[\"Entity\"], prefix=\"Entity\")\n",
    "\n",
    "#Join these new columns back into the DataFrame\n",
    "df = pd.DataFrame()\n",
    "df = df.join(onehot)\n",
    "\n",
    "\n",
    "##Test\n",
    "onehot = pd.get_dummies(test_df[\"Entity\"], prefix=\"Entity\")\n",
    "\n",
    "test_df = test_df.join(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enrich using TextBlob's built in sentiment analysis\n",
    "##Train\n",
    "df[\"Polarity\"], df[\"Subjectivity\"] = ut.tb_enrichtb_enrich(list(df[\"Tweet_Content\"]))\n",
    "\n",
    "\n",
    "##Test\n",
    "test_df[\"Polarity\"], test_df[\"Subjectivity\"] = ut.tb_enrichtb_enrich(list(test_df[\"Tweet_Content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the \"Sentiment\" column into indexes\n",
    "\n",
    "##Train\n",
    "df[\"Sentiment\"] = ut.names_to_ids(all_df[\"Sentiment\"])\n",
    "y = df[\"Sentiment\"]\n",
    "\n",
    "##Test\n",
    "test_df[\"Sentiment\"] = ut.names_to_ids(test_df[\"Sentiment\"])\n",
    "y_test = test_df[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Data Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenisation\n",
    "\n",
    "#Define the Tokeniser\n",
    "tokeniser = Tokenizer(num_words=1000, lower=True)\n",
    "\n",
    "#Create the corpus by finding the most common \n",
    "tokeniser.fit_on_texts(all_df[\"Tweet_Content_Split\"])\n",
    "\n",
    "##Train\n",
    "#Tokenise our column of edited Tweet content\n",
    "nps_tokens = tokeniser.texts_to_matrix(list(all_df[\"Tweet_Content_Split\"]))\n",
    "\n",
    "##Test\n",
    "#Tokenise our column of edited Tweet content\n",
    "nps_tokens_test = tokeniser.texts_to_matrix(list(test_df[\"Tweet_Content_Split\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the dataframe with the tokens using pd.concat\n",
    "\n",
    "#Reset axes to avoid overlapping\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "##Train\n",
    "full_df = pd.concat([df, pd.DataFrame(nps_tokens)], sort=False, axis=1)\n",
    "\n",
    "##Test\n",
    "full_test_df = pd.concat([test_df, pd.DataFrame(nps_tokens_test)], sort=False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final prep\n",
    "\n",
    "##Train\n",
    "#Drop all non-useful columns\n",
    "full_df = full_df.drop([\"Sentiment\", \"Tweet_ID\", \"Tweet_Content\", \"Tweet_Content_Split\", \"Entity\"], axis=1)\n",
    "\n",
    "\n",
    "##Test\n",
    "full_test_df = full_test_df.drop([\"Sentiment\", \"Tweet_ID\", \"Tweet_Content\", \"Tweet_Content_Split\", \"Entity\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition and Training\n",
    "\n",
    "This time, we train with all of the available training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(12, input_dim=1034, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "     loss='sparse_categorical_crossentropy',\n",
    "     optimizer='adam',\n",
    "     metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(\n",
    "     full_df, y,\n",
    "     epochs=30,\n",
    "     callbacks=[tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions\n",
    "y_pred = np.argmax(model.predict(full_test_df), axis=1)\n",
    "\n",
    "#Assign labels to predictions and test data\n",
    "y_pred_labels = ut.ids_to_names(y_pred)\n",
    "y_test_labels = ut.ids_to_names(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique = list(set(y_test_labels))\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels, labels = y_unique, normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y_unique)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see the final accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original\n",
    "model2 = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Embedding(10000,12,input_length=50),\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
    "tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "     loss='sparse_categorical_crossentropy',\n",
    "     optimizer='adam',\n",
    "     metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model2.fit(\n",
    "     full_df, y,\n",
    "     validation_data=(full_test_df, y_test),\n",
    "     epochs=30,\n",
    "     callbacks=[tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f2ae5d3a08a6e204c8c8cee8069ee9bbd2ab88ccb9eee13930db12104613cdbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
