{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032369,
     "end_time": "2020-10-06T21:19:31.851445",
     "exception": false,
     "start_time": "2020-10-06T21:19:31.819076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "This project represents an attempt to utilize machine learning and natural language processing techniques to predict NPS sentiments (Promotor, Passive, Detractor) based on surveys submitted through the NPS system. \n",
    "\n",
    "Author: Eric G. Suchanek, PhD for BestBuy.\n",
    "\n",
    "\n",
    "Methods tried: \n",
    "* Sentiment analysis using textblob\n",
    "* RNN\n",
    "* LSTM\n",
    "* BERT\n",
    "* BoW\n",
    "\n",
    "The following directory structure must be maintained:  \n",
    "* main directory/\n",
    "* notebook/  <-- where this notebook resides\n",
    "    --data/\n",
    "    -- raw/\n",
    "    -- clean/\n",
    "    -- pass/\n",
    "    -- prom/\n",
    "    -- det/\n",
    "\n",
    "Based on: https://erleem.medium.com/nlp-complete-sentiment-analysis-on-amazon-reviews-374e4fea9976\n",
    "\n",
    "(c) 2022 BestBuy, all rights reserved. Confidential. Do not share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030876,
     "end_time": "2020-10-06T21:19:31.915205",
     "exception": false,
     "start_time": "2020-10-06T21:19:31.884329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'prompt_toolkit.formatted_text'. Consider installing this module.\n",
      "Click <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "import bby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:31.986613Z",
     "iopub.status.busy": "2020-10-06T21:19:31.985943Z",
     "iopub.status.idle": "2020-10-06T21:19:41.514922Z",
     "shell.execute_reply": "2020-10-06T21:19:41.515686Z"
    },
    "papermill": {
     "duration": 9.569887,
     "end_time": "2020-10-06T21:19:41.515898",
     "exception": false,
     "start_time": "2020-10-06T21:19:31.946011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "import datetime\n",
    "\n",
    "from bby.util import lemma_remove_stopwords\n",
    "from bby.util import sent_to_words, get_wordnet_pos, detokenize\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032245,
     "end_time": "2020-10-06T21:19:41.583484",
     "exception": false,
     "start_time": "2020-10-06T21:19:41.551239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data importing and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:41.667156Z",
     "iopub.status.busy": "2020-10-06T21:19:41.666425Z",
     "iopub.status.idle": "2020-10-06T21:19:41.753917Z",
     "shell.execute_reply": "2020-10-06T21:19:41.752384Z"
    },
    "papermill": {
     "duration": 0.132806,
     "end_time": "2020-10-06T21:19:41.754037",
     "exception": false,
     "start_time": "2020-10-06T21:19:41.621231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Workforce</th>\n",
       "      <th>NPS® Breakdown</th>\n",
       "      <th>respid2</th>\n",
       "      <th>NPS_Code</th>\n",
       "      <th>NPSCommentCleaned</th>\n",
       "      <th>NPSCommentLemmatised</th>\n",
       "      <th>NPSCommentPolarity</th>\n",
       "      <th>NPSCommentSubjectivity</th>\n",
       "      <th>OverallCommentCleaned</th>\n",
       "      <th>OverallCommentLemmatised</th>\n",
       "      <th>OverallCommentPolarity</th>\n",
       "      <th>OverallCommentSubjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1763</td>\n",
       "      <td>Precinct</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>7053157</td>\n",
       "      <td>2</td>\n",
       "      <td>staff in store in person close by when need them</td>\n",
       "      <td>Staff in store, in person, close by when I nee...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>was told the issue is more related to gmail th...</td>\n",
       "      <td>Was told the issue is more related to gmail th...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>832</td>\n",
       "      <td>Precinct</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>6984308</td>\n",
       "      <td>2</td>\n",
       "      <td>adieb anbari was beyond helpful he answered al...</td>\n",
       "      <td>Adieb Anbari was beyond helpful . He answered ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>adieb was great would recommend him to help an...</td>\n",
       "      <td>Adieb was great I would recommend him to help ...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>288</td>\n",
       "      <td>Precinct</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>6980586</td>\n",
       "      <td>2</td>\n",
       "      <td>quick and knowledgeable</td>\n",
       "      <td>Quick and knowledgeable</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>xyxyxz</td>\n",
       "      <td>xyxyxz</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168</td>\n",
       "      <td>Precinct</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>6789897</td>\n",
       "      <td>2</td>\n",
       "      <td>he called back quickly within minutes and was ...</td>\n",
       "      <td>He called back quickly (within 5 minutes) and ...</td>\n",
       "      <td>0.414444</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>appreciate his quick and knowledgeable response</td>\n",
       "      <td>Appreciate his quick and knowledgeable response.</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>836</td>\n",
       "      <td>Precinct</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>6896172</td>\n",
       "      <td>2</td>\n",
       "      <td>had really good experience thanks to your tech...</td>\n",
       "      <td>I had a really good experience thanks to your ...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>ricky finished with his prior appt so he took ...</td>\n",
       "      <td>Ricky finished with his prior appt so he took ...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Location Workforce NPS® Breakdown  respid2  NPS_Code  \\\n",
       "0      1763  Precinct       Promoter  7053157         2   \n",
       "1       832  Precinct       Promoter  6984308         2   \n",
       "2       288  Precinct       Promoter  6980586         2   \n",
       "3       168  Precinct       Promoter  6789897         2   \n",
       "4       836  Precinct       Promoter  6896172         2   \n",
       "\n",
       "                                   NPSCommentCleaned  \\\n",
       "0   staff in store in person close by when need them   \n",
       "1  adieb anbari was beyond helpful he answered al...   \n",
       "2                            quick and knowledgeable   \n",
       "3  he called back quickly within minutes and was ...   \n",
       "4  had really good experience thanks to your tech...   \n",
       "\n",
       "                                NPSCommentLemmatised  NPSCommentPolarity  \\\n",
       "0  Staff in store, in person, close by when I nee...            0.000000   \n",
       "1  Adieb Anbari was beyond helpful . He answered ...            0.100000   \n",
       "2                            Quick and knowledgeable            0.333333   \n",
       "3  He called back quickly (within 5 minutes) and ...            0.414444   \n",
       "4  I had a really good experience thanks to your ...            0.450000   \n",
       "\n",
       "   NPSCommentSubjectivity                              OverallCommentCleaned  \\\n",
       "0                0.000000  was told the issue is more related to gmail th...   \n",
       "1                0.600000  adieb was great would recommend him to help an...   \n",
       "2                0.500000                                             xyxyxz   \n",
       "3                0.426667    appreciate his quick and knowledgeable response   \n",
       "4                0.400000  ricky finished with his prior appt so he took ...   \n",
       "\n",
       "                            OverallCommentLemmatised  OverallCommentPolarity  \\\n",
       "0  Was told the issue is more related to gmail th...                0.250000   \n",
       "1  Adieb was great I would recommend him to help ...                0.800000   \n",
       "2                                             xyxyxz                0.000000   \n",
       "3   Appreciate his quick and knowledgeable response.                0.333333   \n",
       "4  Ricky finished with his prior appt so he took ...                0.050000   \n",
       "\n",
       "   OverallCommentSubjectivity  \n",
       "0                        0.45  \n",
       "1                        0.75  \n",
       "2                        0.00  \n",
       "3                        0.50  \n",
       "4                        0.15  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NPS_df = pd.read_csv('../data/clean/NPS_NATL_subset.csv')\n",
    "#NPS_df = pd.read_csv('../data/clean/NPS_NATL_small.csv')\n",
    "NPS_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:41.980569Z",
     "iopub.status.busy": "2020-10-06T21:19:41.979730Z",
     "iopub.status.idle": "2020-10-06T21:19:41.983566Z",
     "shell.execute_reply": "2020-10-06T21:19:41.983073Z"
    },
    "papermill": {
     "duration": 0.041264,
     "end_time": "2020-10-06T21:19:41.983666",
     "exception": false,
     "start_time": "2020-10-06T21:19:41.942402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26703"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's get the dataset lenght\n",
    "len(NPS_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:42.143106Z",
     "iopub.status.busy": "2020-10-06T21:19:42.142318Z",
     "iopub.status.idle": "2020-10-06T21:19:42.185676Z",
     "shell.execute_reply": "2020-10-06T21:19:42.186310Z"
    },
    "papermill": {
     "duration": 0.088183,
     "end_time": "2020-10-06T21:19:42.186453",
     "exception": false,
     "start_time": "2020-10-06T21:19:42.098270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Workforce</th>\n",
       "      <th>respid2</th>\n",
       "      <th>NPS_Code</th>\n",
       "      <th>NPSCommentCleaned</th>\n",
       "      <th>NPSCommentLemmatised</th>\n",
       "      <th>NPSCommentPolarity</th>\n",
       "      <th>NPSCommentSubjectivity</th>\n",
       "      <th>OverallCommentCleaned</th>\n",
       "      <th>OverallCommentLemmatised</th>\n",
       "      <th>OverallCommentPolarity</th>\n",
       "      <th>OverallCommentSubjectivity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPS® Breakdown</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Detractor</th>\n",
       "      <td>928</td>\n",
       "      <td>2</td>\n",
       "      <td>10017</td>\n",
       "      <td>1</td>\n",
       "      <td>9778</td>\n",
       "      <td>9881</td>\n",
       "      <td>2267</td>\n",
       "      <td>2254</td>\n",
       "      <td>6999</td>\n",
       "      <td>7072</td>\n",
       "      <td>1942</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passive</th>\n",
       "      <td>925</td>\n",
       "      <td>2</td>\n",
       "      <td>7785</td>\n",
       "      <td>1</td>\n",
       "      <td>7295</td>\n",
       "      <td>7463</td>\n",
       "      <td>1327</td>\n",
       "      <td>1190</td>\n",
       "      <td>4126</td>\n",
       "      <td>4206</td>\n",
       "      <td>1176</td>\n",
       "      <td>1133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Promoter</th>\n",
       "      <td>929</td>\n",
       "      <td>2</td>\n",
       "      <td>8901</td>\n",
       "      <td>1</td>\n",
       "      <td>7780</td>\n",
       "      <td>8099</td>\n",
       "      <td>995</td>\n",
       "      <td>863</td>\n",
       "      <td>4306</td>\n",
       "      <td>4509</td>\n",
       "      <td>932</td>\n",
       "      <td>863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Location  Workforce  respid2  NPS_Code  NPSCommentCleaned  \\\n",
       "NPS® Breakdown                                                              \n",
       "Detractor            928          2    10017         1               9778   \n",
       "Passive              925          2     7785         1               7295   \n",
       "Promoter             929          2     8901         1               7780   \n",
       "\n",
       "                NPSCommentLemmatised  NPSCommentPolarity  \\\n",
       "NPS® Breakdown                                             \n",
       "Detractor                       9881                2267   \n",
       "Passive                         7463                1327   \n",
       "Promoter                        8099                 995   \n",
       "\n",
       "                NPSCommentSubjectivity  OverallCommentCleaned  \\\n",
       "NPS® Breakdown                                                  \n",
       "Detractor                         2254                   6999   \n",
       "Passive                           1190                   4126   \n",
       "Promoter                           863                   4306   \n",
       "\n",
       "                OverallCommentLemmatised  OverallCommentPolarity  \\\n",
       "NPS® Breakdown                                                     \n",
       "Detractor                           7072                    1942   \n",
       "Passive                             4206                    1176   \n",
       "Promoter                            4509                     932   \n",
       "\n",
       "                OverallCommentSubjectivity  \n",
       "NPS® Breakdown                              \n",
       "Detractor                             1963  \n",
       "Passive                               1133  \n",
       "Promoter                               863  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How's distributed the dataset? Is it biased?\n",
    "NPS_df.groupby('NPS® Breakdown').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Checking balance of target classes\n",
    "sentiments = list(NPS_df[\"NPS® Breakdown\"].unique())\n",
    "\n",
    "sentiment_nums = [len(NPS_df[NPS_df[\"NPS® Breakdown\"] == sentiment]) / len(NPS_df) for sentiment in sentiments]\n",
    "\n",
    "plt.bar(sentiments, sentiment_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033994,
     "end_time": "2020-10-06T21:19:42.255666",
     "exception": false,
     "start_time": "2020-10-06T21:19:42.221672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['staff in store in person close by when need them', 'adieb anbari was beyond helpful he answered all my questions got me in and out of there as fast as possible and even entertained my year old by answering all her questions and making boat out of paper for her', 'quick and knowledgeable', 'he called back quickly within minutes and was very good at explaining the reason for our issue', 'had really good experience thanks to your tech named ricky']\n"
     ]
    }
   ],
   "source": [
    "# grab the NPS and NPS overall comments for all stores\n",
    "#overall_list = NPS_df['OverallCommentLemmatised'].apply(lemma_remove_stopwords)\n",
    "\n",
    "\n",
    "def str_it(_ls):\n",
    "    ls = str(_ls)\n",
    "    word_tokens = word_tokenize(ls)\n",
    "    ls = [w for w in word_tokens]\n",
    "\n",
    "    ls = \" \".join(ls)\n",
    "    return ls\n",
    "    \n",
    "\n",
    "nps_list = NPS_df['NPSCommentCleaned'].apply(str_it)\n",
    "data_words = list(sent_to_words(nps_list))\n",
    "len(data_words)\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(len(data_words)):\n",
    "    data.append(detokenize(data_words[i]))\n",
    "print(data[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037183,
     "end_time": "2020-10-06T21:19:48.013459",
     "exception": false,
     "start_time": "2020-10-06T21:19:47.976276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Label encoding\n",
    "\n",
    "As the dataset is categorical, we need to convert the sentiment labels from Detractor, Passive and Promoter to a float type that our model can understand. To achieve this task, we'll implement the to_categorical method from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Workforce</th>\n",
       "      <th>NPS® Breakdown</th>\n",
       "      <th>respid2</th>\n",
       "      <th>NPS_Code</th>\n",
       "      <th>NPSCommentCleaned</th>\n",
       "      <th>NPSCommentLemmatised</th>\n",
       "      <th>NPSCommentPolarity</th>\n",
       "      <th>NPSCommentSubjectivity</th>\n",
       "      <th>OverallCommentCleaned</th>\n",
       "      <th>OverallCommentLemmatised</th>\n",
       "      <th>OverallCommentPolarity</th>\n",
       "      <th>OverallCommentSubjectivity</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1763</td>\n",
       "      <td>Precinct</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>7053157</td>\n",
       "      <td>2</td>\n",
       "      <td>staff in store in person close by when need them</td>\n",
       "      <td>Staff in store, in person, close by when I nee...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>was told the issue is more related to gmail th...</td>\n",
       "      <td>Was told the issue is more related to gmail th...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>832</td>\n",
       "      <td>Precinct</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>6984308</td>\n",
       "      <td>2</td>\n",
       "      <td>adieb anbari was beyond helpful he answered al...</td>\n",
       "      <td>Adieb Anbari was beyond helpful . He answered ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>adieb was great would recommend him to help an...</td>\n",
       "      <td>Adieb was great I would recommend him to help ...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>288</td>\n",
       "      <td>Precinct</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>6980586</td>\n",
       "      <td>2</td>\n",
       "      <td>quick and knowledgeable</td>\n",
       "      <td>Quick and knowledgeable</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>xyxyxz</td>\n",
       "      <td>xyxyxz</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168</td>\n",
       "      <td>Precinct</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>6789897</td>\n",
       "      <td>2</td>\n",
       "      <td>he called back quickly within minutes and was ...</td>\n",
       "      <td>He called back quickly (within 5 minutes) and ...</td>\n",
       "      <td>0.414444</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>appreciate his quick and knowledgeable response</td>\n",
       "      <td>Appreciate his quick and knowledgeable response.</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>836</td>\n",
       "      <td>Precinct</td>\n",
       "      <td>Promoter</td>\n",
       "      <td>6896172</td>\n",
       "      <td>2</td>\n",
       "      <td>had really good experience thanks to your tech...</td>\n",
       "      <td>I had a really good experience thanks to your ...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>ricky finished with his prior appt so he took ...</td>\n",
       "      <td>Ricky finished with his prior appt so he took ...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Location Workforce NPS® Breakdown  respid2  NPS_Code  \\\n",
       "0      1763  Precinct       Promoter  7053157         2   \n",
       "1       832  Precinct       Promoter  6984308         2   \n",
       "2       288  Precinct       Promoter  6980586         2   \n",
       "3       168  Precinct       Promoter  6789897         2   \n",
       "4       836  Precinct       Promoter  6896172         2   \n",
       "\n",
       "                                   NPSCommentCleaned  \\\n",
       "0   staff in store in person close by when need them   \n",
       "1  adieb anbari was beyond helpful he answered al...   \n",
       "2                            quick and knowledgeable   \n",
       "3  he called back quickly within minutes and was ...   \n",
       "4  had really good experience thanks to your tech...   \n",
       "\n",
       "                                NPSCommentLemmatised  NPSCommentPolarity  \\\n",
       "0  Staff in store, in person, close by when I nee...            0.000000   \n",
       "1  Adieb Anbari was beyond helpful . He answered ...            0.100000   \n",
       "2                            Quick and knowledgeable            0.333333   \n",
       "3  He called back quickly (within 5 minutes) and ...            0.414444   \n",
       "4  I had a really good experience thanks to your ...            0.450000   \n",
       "\n",
       "   NPSCommentSubjectivity                              OverallCommentCleaned  \\\n",
       "0                0.000000  was told the issue is more related to gmail th...   \n",
       "1                0.600000  adieb was great would recommend him to help an...   \n",
       "2                0.500000                                             xyxyxz   \n",
       "3                0.426667    appreciate his quick and knowledgeable response   \n",
       "4                0.400000  ricky finished with his prior appt so he took ...   \n",
       "\n",
       "                            OverallCommentLemmatised  OverallCommentPolarity  \\\n",
       "0  Was told the issue is more related to gmail th...                0.250000   \n",
       "1  Adieb was great I would recommend him to help ...                0.800000   \n",
       "2                                             xyxyxz                0.000000   \n",
       "3   Appreciate his quick and knowledgeable response.                0.333333   \n",
       "4  Ricky finished with his prior appt so he took ...                0.050000   \n",
       "\n",
       "   OverallCommentSubjectivity  code  \n",
       "0                        0.45     2  \n",
       "1                        0.75     2  \n",
       "2                        0.00     2  \n",
       "3                        0.50     2  \n",
       "4                        0.15     2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE = LabelEncoder()\n",
    "NPS_df['code'] = LE.fit_transform(NPS_df['NPS® Breakdown'])\n",
    "NPS_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:48.122784Z",
     "iopub.status.busy": "2020-10-06T21:19:48.121518Z",
     "iopub.status.idle": "2020-10-06T21:19:48.125042Z",
     "shell.execute_reply": "2020-10-06T21:19:48.125481Z"
    },
    "papermill": {
     "duration": 0.074223,
     "end_time": "2020-10-06T21:19:48.125627",
     "exception": false,
     "start_time": "2020-10-06T21:19:48.051404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26703\n"
     ]
    }
   ],
   "source": [
    "olabels = np.array(NPS_df['NPS® Breakdown'])\n",
    "labels = np.array(NPS_df['NPS® Breakdown'])\n",
    "\n",
    "y = NPS_df['NPS_Code'].values.tolist()\n",
    "\n",
    "#for i in labels:\n",
    "#    if i == 'Promoter':\n",
    "#        y.append(2.0)\n",
    "#    elif i == 'Passive':\n",
    "#        y.append(1.0)\n",
    "#    elif i == 'Detractor':\n",
    "#        y.append(0.0)\n",
    "\n",
    "y = np.array(y)\n",
    "labels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\n",
    "del y\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038298,
     "end_time": "2020-10-06T21:19:48.287312",
     "exception": false,
     "start_time": "2020-10-06T21:19:48.249014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data sequencing and splitting\n",
    "\n",
    "We'll implement the Keras tokenizer as well as its pad_sequences method to transform our text data into 3D float data, otherwise our neural networks won't be able to be trained on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:48.373707Z",
     "iopub.status.busy": "2020-10-06T21:19:48.372469Z",
     "iopub.status.idle": "2020-10-06T21:19:49.386819Z",
     "shell.execute_reply": "2020-10-06T21:19:49.387562Z"
    },
    "papermill": {
     "duration": 1.062302,
     "end_time": "2020-10-06T21:19:49.387739",
     "exception": false,
     "start_time": "2020-10-06T21:19:48.325437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "max_words = 20000\n",
    "# was 5000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "comments = pad_sequences(sequences, maxlen=max_len)\n",
    "#\n",
    "#  use scikit-multilearn since we are using a non-binary (trinary output)\n",
    "import skmultilearn\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "X_train, y_train, X_test, y_test = iterative_train_test_split(comments, labels, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.044723,
     "end_time": "2020-10-06T21:19:49.690338",
     "exception": false,
     "start_time": "2020-10-06T21:19:49.645615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model building\n",
    "\n",
    "In the next cells we experiment with several different Neural Networks. I'll implement sequential models from the Keras API to achieve this task. Essentially, I'll start with a single layer **LSTM** network which is known by achieving good results in NLP tasks when the dataset is relatively small (I could have started with a SimpleRNN which is even simpler, but to be honest it's actually not deployed in production environments because it is too simple - however I'll leave it commented in case you want to know it's built). The next one will be a Bidirectional LSTM model, a more complex one and this particular one is known to achieve great metrics when talking about text classification. To go beyond the classic NLP approach, finally we'll implement a very unusual model: a Convolutional 1D network, known as well by delivering good metrics when talking about NLP. If everything goes ok, we should get the best results with the BidRNN, let's see what happens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.045363,
     "end_time": "2020-10-06T21:19:49.779234",
     "exception": false,
     "start_time": "2020-10-06T21:19:49.733871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SimpleRNN model (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:49.876465Z",
     "iopub.status.busy": "2020-10-06T21:19:49.875466Z",
     "iopub.status.idle": "2020-10-06T21:19:49.878789Z",
     "shell.execute_reply": "2020-10-06T21:19:49.878237Z"
    },
    "papermill": {
     "duration": 0.054472,
     "end_time": "2020-10-06T21:19:49.878902",
     "exception": false,
     "start_time": "2020-10-06T21:19:49.824430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 20:27:15.139750: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-06 20:27:15.139991: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-07-06 20:27:15.342313: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-07-06 20:27:16.225243: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50/668 [=>............................] - ETA: 12:03:49 - loss: 1.0854 - accuracy: 0.3925"
     ]
    }
   ],
   "source": [
    "model0 = Sequential()\n",
    "model0.add(layers.Embedding(max_words, 15))\n",
    "model0.add(layers.SimpleRNN(15))\n",
    "model0.add(layers.Dense(3,activation='softmax'))\n",
    "\n",
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "metric = 'val_accuracy'\n",
    "model0.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint0 = ModelCheckpoint(\"best_model0.hdf5\", monitor='accuracy', verbose=0, save_best_only=True, mode='auto', save_freq=1, save_weights_only=False)\n",
    "history = model0.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[checkpoint0, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "pyplot.title('Learning Curves')\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Cross Entropy')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.044937,
     "end_time": "2020-10-06T21:19:49.968644",
     "exception": false,
     "start_time": "2020-10-06T21:19:49.923707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Single LSTM layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:50.068632Z",
     "iopub.status.busy": "2020-10-06T21:19:50.067909Z",
     "iopub.status.idle": "2020-10-06T21:30:02.803265Z",
     "shell.execute_reply": "2020-10-06T21:30:02.805203Z"
    },
    "papermill": {
     "duration": 612.793273,
     "end_time": "2020-10-06T21:30:02.806611",
     "exception": false,
     "start_time": "2020-10-06T21:19:50.013338",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/suchanek/repos/npsML/notebooks/NPSanalysis_keras_working3.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPSanalysis_keras_working3.ipynb#ch0000002?line=0'>1</a>\u001b[0m model1 \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPSanalysis_keras_working3.ipynb#ch0000002?line=1'>2</a>\u001b[0m model1\u001b[39m.\u001b[39madd(Embedding(max_words, \u001b[39m20\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/suchanek/repos/npsML/notebooks/NPSanalysis_keras_working3.ipynb#ch0000002?line=2'>3</a>\u001b[0m model1\u001b[39m.\u001b[39madd(LSTM(\u001b[39m15\u001b[39m,dropout\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)) \u001b[39m# was .5\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(max_words, 20))\n",
    "model1.add(LSTM(15,dropout=0.5)) # was .5\n",
    "model1.add(Dense(3,activation='softmax'))\n",
    "\n",
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='loss', verbose=0, save_best_only=True, mode='auto', save_freq=1,save_weights_only=False)\n",
    "history = model1.fit(X_train, y_train, epochs=50,validation_data=(X_test, y_test),callbacks=[checkpoint1, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:50.068632Z",
     "iopub.status.busy": "2020-10-06T21:19:50.067909Z",
     "iopub.status.idle": "2020-10-06T21:30:02.803265Z",
     "shell.execute_reply": "2020-10-06T21:30:02.805203Z"
    },
    "papermill": {
     "duration": 612.793273,
     "end_time": "2020-10-06T21:30:02.806611",
     "exception": false,
     "start_time": "2020-10-06T21:19:50.013338",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is generally about the best model so far 5/16/22\n",
    "model1b = Sequential()\n",
    "model1b.add(Embedding(max_words, 20))\n",
    "model1b.add(LSTM(25,dropout=0.6))\n",
    "model1b.add(Dense(10,activation='tanh'))\n",
    "model1b.add(Dense(3,activation='softmax'))\n",
    "\n",
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "model1b.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint4 = ModelCheckpoint(\"best_model1b.hdf5\", monitor='loss', verbose=0, save_best_only=True, mode='auto', save_freq=1,save_weights_only=False)\n",
    "history = model1b.fit(X_train, y_train, epochs=50,validation_data=(X_test, y_test),callbacks=[checkpoint4, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:50.068632Z",
     "iopub.status.busy": "2020-10-06T21:19:50.067909Z",
     "iopub.status.idle": "2020-10-06T21:30:02.803265Z",
     "shell.execute_reply": "2020-10-06T21:30:02.805203Z"
    },
    "papermill": {
     "duration": 612.793273,
     "end_time": "2020-10-06T21:30:02.806611",
     "exception": false,
     "start_time": "2020-10-06T21:19:50.013338",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1c = Sequential()\n",
    "model1c.add(Embedding(max_words, 100))\n",
    "model1c.add(LSTM(25,dropout=0.6))\n",
    "model1c.add(Dense(10,activation='relu'))\n",
    "model1c.add(Dense(3,activation='softmax'))\n",
    "\n",
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "model1c.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint5 = ModelCheckpoint(\"best_model1c.hdf5\", monitor='loss', verbose=0, save_best_only=True, mode='auto', save_freq=1,save_weights_only=False)\n",
    "history = model1c.fit(X_train, y_train, epochs=50,validation_data=(X_test, y_test),callbacks=[checkpoint5, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "pyplot.title('Learning Curves')\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Cross Entropy')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.837682,
     "end_time": "2020-10-06T21:30:10.817414",
     "exception": false,
     "start_time": "2020-10-06T21:30:06.979732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bidirectional LTSM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:30:18.875895Z",
     "iopub.status.busy": "2020-10-06T21:30:18.874621Z",
     "iopub.status.idle": "2020-10-06T21:49:59.410271Z",
     "shell.execute_reply": "2020-10-06T21:49:59.411560Z"
    },
    "papermill": {
     "duration": 1184.636954,
     "end_time": "2020-10-06T21:49:59.411827",
     "exception": false,
     "start_time": "2020-10-06T21:30:14.774873",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_words, 100, input_length=max_len))\n",
    "model2.add(Bidirectional(LSTM(20,dropout=0.6)))\n",
    "model2.add(Dense(10,activation='sigmoid'))\n",
    "model2.add(Dense(3,activation='softmax'))\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='accuracy', verbose=0, save_best_only=True, mode='auto', save_freq=1,save_weights_only=False)\n",
    "history = model2.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[checkpoint2, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "\n",
    "pyplot.title('Learning Curves')\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Cross Entropy')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 11.452373,
     "end_time": "2020-10-06T21:50:21.772094",
     "exception": false,
     "start_time": "2020-10-06T21:50:10.319721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1D Convolutional model\n",
    "\n",
    "Before diving into this model, I know by prior experience that it tends to overfit extremely fast on small datasets. In this sense, just will implement it to show you how to do it in case it's of your interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:50:43.900261Z",
     "iopub.status.busy": "2020-10-06T21:50:43.899061Z",
     "iopub.status.idle": "2020-10-06T21:56:48.149729Z",
     "shell.execute_reply": "2020-10-06T21:56:48.149055Z"
    },
    "papermill": {
     "duration": 375.351238,
     "end_time": "2020-10-06T21:56:48.149913",
     "exception": false,
     "start_time": "2020-10-06T21:50:32.798675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "from keras import regularizers\n",
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(layers.Embedding(max_words, 100, input_length=max_len)) # was 40\n",
    "model3.add(layers.Conv1D(20, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\n",
    "model3.add(layers.MaxPooling1D(5))\n",
    "model3.add(layers.Conv1D(20, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\n",
    "model3.add(layers.GlobalMaxPooling1D())\n",
    "model3.add(layers.Dense(3,activation='softmax'))\n",
    "model3.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpoint3 = ModelCheckpoint(\"best_model3.hdf5\", monitor='accuracy', verbose=0, save_best_only=True, mode='auto', save_freq=1,save_weights_only=False)\n",
    "history = model3.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test),callbacks=[checkpoint3, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "pyplot.title('Learning Curves')\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('Cross Entropy')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 13.181114,
     "end_time": "2020-10-06T21:57:15.037099",
     "exception": false,
     "start_time": "2020-10-06T21:57:01.855985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you check the val_accuracy metric in the training logs you won't find better score than the one achieved by the BidRNN. Again, the previous model is not the best for this task becaue is majorly used for short translation tasks, but the good thing to notice is its speed to train.\n",
    "\n",
    "Let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 13.897718,
     "end_time": "2020-10-06T21:57:43.244943",
     "exception": false,
     "start_time": "2020-10-06T21:57:29.347225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Best model validation\n",
    "(Before final commit, the best model obtained was the BidRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:58:11.467383Z",
     "iopub.status.busy": "2020-10-06T21:58:11.466210Z",
     "iopub.status.idle": "2020-10-06T21:58:12.123269Z",
     "shell.execute_reply": "2020-10-06T21:58:12.122665Z"
    },
    "papermill": {
     "duration": 14.579894,
     "end_time": "2020-10-06T21:58:12.123401",
     "exception": false,
     "start_time": "2020-10-06T21:57:57.543507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's load the best model obtained during training\n",
    "best_model = keras.models.load_model(\"best_model4.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:58:39.201557Z",
     "iopub.status.busy": "2020-10-06T21:58:39.200429Z",
     "iopub.status.idle": "2020-10-06T21:58:42.153698Z",
     "shell.execute_reply": "2020-10-06T21:58:42.152736Z"
    },
    "papermill": {
     "duration": 16.163436,
     "end_time": "2020-10-06T21:58:42.153869",
     "exception": false,
     "start_time": "2020-10-06T21:58:25.990433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Model accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:59:08.598704Z",
     "iopub.status.busy": "2020-10-06T21:59:08.597660Z",
     "iopub.status.idle": "2020-10-06T21:59:11.143379Z",
     "shell.execute_reply": "2020-10-06T21:59:11.144225Z"
    },
    "papermill": {
     "duration": 15.873978,
     "end_time": "2020-10-06T21:59:11.144400",
     "exception": false,
     "start_time": "2020-10-06T21:58:55.270422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 13.54467,
     "end_time": "2020-10-06T21:59:38.859073",
     "exception": false,
     "start_time": "2020-10-06T21:59:25.314403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Confusion matrix\n",
    "\n",
    "Alright, we all know the accuracy is not a good metric to measure how well a model is. That's the reason why I like to always see its confusion matrix, that way I have a better understanding of its classification and generalization ability. Let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:00:05.244627Z",
     "iopub.status.busy": "2020-10-06T22:00:05.243340Z",
     "iopub.status.idle": "2020-10-06T22:00:05.261173Z",
     "shell.execute_reply": "2020-10-06T22:00:05.260523Z"
    },
    "papermill": {
     "duration": 13.356845,
     "end_time": "2020-10-06T22:00:05.261290",
     "exception": false,
     "start_time": "2020-10-06T21:59:51.904445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_test.argmax(axis=1), np.around(predictions, decimals=0).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:00:32.586908Z",
     "iopub.status.busy": "2020-10-06T22:00:32.586210Z",
     "iopub.status.idle": "2020-10-06T22:00:33.285612Z",
     "shell.execute_reply": "2020-10-06T22:00:33.285009Z"
    },
    "papermill": {
     "duration": 14.015558,
     "end_time": "2020-10-06T22:00:33.285736",
     "exception": false,
     "start_time": "2020-10-06T22:00:19.270178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "conf_matrix = pd.DataFrame(matrix, index = ['Detractor','Passive','Promoter'],columns = ['Detractor','Passive','Promoter'])\n",
    "#Normalizing\n",
    "conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize = (15,15))\n",
    "sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 12.906403,
     "end_time": "2020-10-06T22:00:59.275554",
     "exception": false,
     "start_time": "2020-10-06T22:00:46.369151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Again, the model's score is very poor, but keep in mind it hasn't gone through hyperparameter tuning. Let's see how it performs on some test text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'prompt_toolkit.formatted_text'. Consider installing this module.\n",
      "Click <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "def predict_nps(input_sentence, data, model, max_words, max_len):\n",
    "    sentiment = ['Detractor','Passive','Promoter']\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    sequence = tokenizer.texts_to_sequences([input_sentence])\n",
    "    test = pad_sequences(sequence, maxlen=max_len)\n",
    "    print(f'{input_sentence} - {sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]}')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:01:26.949303Z",
     "iopub.status.busy": "2020-10-06T22:01:26.947906Z",
     "iopub.status.idle": "2020-10-06T22:01:26.951809Z",
     "shell.execute_reply": "2020-10-06T22:01:26.951229Z"
    },
    "papermill": {
     "duration": 14.303017,
     "end_time": "2020-10-06T22:01:26.951936",
     "exception": false,
     "start_time": "2020-10-06T22:01:12.648919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_nps('Eric Suchanek is the best agent ever!', data, best_model, max_words, max_len)\n",
    "predict_nps('i hate youtube ads, they are annoying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:01:53.611240Z",
     "iopub.status.busy": "2020-10-06T22:01:53.610198Z",
     "iopub.status.idle": "2020-10-06T22:01:53.680730Z",
     "shell.execute_reply": "2020-10-06T22:01:53.679943Z"
    },
    "papermill": {
     "duration": 13.274625,
     "end_time": "2020-10-06T22:01:53.680880",
     "exception": false,
     "start_time": "2020-10-06T22:01:40.406255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:02:21.215521Z",
     "iopub.status.busy": "2020-10-06T22:02:21.214548Z",
     "iopub.status.idle": "2020-10-06T22:02:21.272639Z",
     "shell.execute_reply": "2020-10-06T22:02:21.271230Z"
    },
    "papermill": {
     "duration": 14.445195,
     "end_time": "2020-10-06T22:02:21.272790",
     "exception": false,
     "start_time": "2020-10-06T22:02:06.827595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['Eric Suchanek is the best agent ever!'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:02:48.026609Z",
     "iopub.status.busy": "2020-10-06T22:02:48.025563Z",
     "iopub.status.idle": "2020-10-06T22:02:48.086453Z",
     "shell.execute_reply": "2020-10-06T22:02:48.085808Z"
    },
    "papermill": {
     "duration": 13.396502,
     "end_time": "2020-10-06T22:02:48.086621",
     "exception": false,
     "start_time": "2020-10-06T22:02:34.690119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['i hate youtube ads, they are annoying'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T22:03:14.961947Z",
     "iopub.status.busy": "2020-10-06T22:03:14.960920Z",
     "iopub.status.idle": "2020-10-06T22:03:15.014373Z",
     "shell.execute_reply": "2020-10-06T22:03:15.013738Z"
    },
    "papermill": {
     "duration": 13.646468,
     "end_time": "2020-10-06T22:03:15.014510",
     "exception": false,
     "start_time": "2020-10-06T22:03:01.368042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['i really loved how the technician helped me with the issue that i had'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "duration": 2724.650172,
   "end_time": "2020-10-06T22:04:52.151005",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-06T21:19:27.500833",
   "version": "2.1.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e9fb08708a348826ea2596edb892bc9ad5e5b82e2f5c7730c1edba9ecbcf32d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
