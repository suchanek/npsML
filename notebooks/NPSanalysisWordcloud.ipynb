{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# NPS comment analysis using NLTK\n",
    "# Author: Eric G. Suchanek, PhD\n",
    "# (c) 2022 BestBuy, All Rights Reserved\n",
    "# Confidential, do not share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Natural Language Processing to better understand NPS Survey Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#TextBlob Features\n",
    "from textblob import TextBlob\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#SciKit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Tensorflow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Test\n",
    "from collections import Counter\n",
    "import bby\n",
    "import bby.util as ut\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Word frequency distributions\n",
    "def nps_freqs(stringlist, howmany=0):\n",
    "    Vocab_str = str()\n",
    "    Vocab_str = \" \".join(str(review).lower() for review in stringlist)\n",
    "\n",
    "    docSplit = Vocab_str.split()\n",
    "    freq = nltk.FreqDist(w for w in docSplit)\n",
    "    return freq\n",
    "\n",
    "def update_vocab(vocab, new_words):\n",
    "    vocab.update(new_words)\n",
    "    return\n",
    "\n",
    "def make_vocab(_vocab, sent, stop_words):\n",
    "    vocab = _vocab\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    for w in word_tokens:\n",
    "        if (len(w) > 2 and w not in stop_words):\n",
    "            vocab.update(w)\n",
    "    return vocab\n",
    "\n",
    "def build_vocabulary(sent_list, stop_words):\n",
    "    vocab = set()\n",
    "    for sent in sent_list:\n",
    "        vocab = make_vocab(vocab, sent, stop_words)\n",
    "    return vocab\n",
    "\n",
    "def make_vocab_strings(stringlist):\n",
    "    Vocab_str = str()\n",
    "    Vocab_str = \" \".join(str(comment).lower() for comment in stringlist)\n",
    "    return Vocab_str\n",
    "\n",
    "                                         \n",
    "def nps_cleanstring(comment):\n",
    "    STOP_english = Counter(stopwords.words()) # Here we use a Counter dictionary on the cached\n",
    "    Vocab_str = comment.lower()\n",
    "    \n",
    "    # print(f'Vocab_string <{Vocab_str}>')\n",
    "    \n",
    "    res = Vocab_str.translate(str.maketrans('', '', string.punctuation)) #removes [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:\n",
    "    docWords = res.split()\n",
    "    filtered_sentence = [w for w in docWords if not w.lower() in STOP_english and len(w) > 1]\n",
    "    \n",
    "    ls = \" \".join(filtered_sentence)\n",
    "    return ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../data/clean/NPS_NATL_cleaned.csv containing a total of: 107625 records.\n",
      "Promoters: 92458, Passives: 6612, Detractors: 8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# National NPS extract processed by CleanNPS_National.py - this cleans and lemmatises the NPS and Overall\n",
    "# comments. The full extract has not been equalized for comment distribution\n",
    "# \n",
    "\n",
    "all_path = \"../data/clean/NPS_NATL_cleaned.csv\"\n",
    "NPS_df = pd.read_csv(all_path)\n",
    "\n",
    "prom_list_mask = NPS_df['NPS_Code'] == 2\n",
    "pass_list_mask = NPS_df['NPS_Code'] == 1\n",
    "det_list_mask = NPS_df['NPS_Code'] == 0\n",
    "\n",
    "prom_df = NPS_df[prom_list_mask].copy()\n",
    "pass_df = NPS_df[pass_list_mask].copy()\n",
    "det_df = NPS_df[det_list_mask].copy()\n",
    "\n",
    "prom_df_len = prom_df.shape[0]\n",
    "pass_df_len = pass_df.shape[0]\n",
    "det_df_len = det_df.shape[0]\n",
    "\n",
    "\n",
    "prom_list_strings = prom_df['NPSCommentCleaned'].values.tolist()\n",
    "pass_list_strings = pass_df['NPSCommentCleaned'].values.tolist()\n",
    "det_list_strings = det_df['NPSCommentCleaned'].values.tolist()\n",
    "\n",
    "prom_df.to_csv('../data/clean/prom_list.csv', index=False)\n",
    "pass_df.to_csv('../data/clean/pass_list.csv', index=False)\n",
    "det_df.to_csv('../data/clean/det_list.csv', index=False)\n",
    "\n",
    "print(f'Read file {all_path} containing a total of: {NPS_df.shape[0]} records.')\n",
    "print (f'Promoters: {prom_df_len}, Passives: {pass_df_len}, Detractors: {det_df_len}')\n",
    "\n",
    "# Rechecking balance of target classes after equalization\n",
    "sentiments = list(NPS_df[\"NPS® Breakdown\"].unique())\n",
    "#sentiment_nums = [len(NPS_df[NPS_df[\"NPS® Breakdown\"] == sentiment]) / len(NPS_df) for sentiment in sentiments]\n",
    "sentiment_nums = [len(NPS_df[NPS_df[\"NPS® Breakdown\"] == sentiment]) for sentiment in sentiments]\n",
    "\n",
    "plt.bar(sentiments, sentiment_nums)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing GS stopwords from promoters.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Build vocabularies for the different classes.\n",
    "#\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from itertools import repeat\n",
    "from itertools import takewhile\n",
    "\n",
    "# I know that the following words are common across all classes so we strip them out in order to\n",
    "# unbias the results\n",
    "\n",
    "GS_stopwords = ['geek', 'squad', 'service', 'customer', 'xyxyxz','\\'']\n",
    "\n",
    "stop_GS = Counter(stopwords.words())\n",
    "stop_GS.update(GS_stopwords)\n",
    "                                      \n",
    "replacements = dict(zip((fr'\\b{word}\\b' for word in stop_GS), repeat(\"\")))\n",
    "\n",
    "# remove the geeksquad 'stopwords' from the dataframes verbatim comments\n",
    "# need for proper frequency analysis\n",
    "\n",
    "print('Removing GS stopwords from promoters.')\n",
    "prom_df.NPSCommentCleaned.replace(replacements, regex=True, inplace=True)\n",
    "prom_df.NPSCommentCleaned.replace({r' +': ' ', r' +\\.': '.'}, regex=True, inplace=True)\n",
    "\n",
    "print('Removing GS stopwords from passives.')\n",
    "pass_df.NPSCommentCleaned.replace(replacements, regex=True, inplace=True)\n",
    "pass_df.NPSCommentCleaned.replace({r' +': ' ', r' +\\.': '.'}, regex=True, inplace=True)\n",
    "\n",
    "print('Removing GS stopwords from detractors.')\n",
    "det_df.NPSCommentCleaned.replace(replacements, regex=True, inplace=True)\n",
    "det_df.NPSCommentCleaned.replace({r' +': ' ', r' +\\.': '.'}, regex=True, inplace=True)\n",
    "\n",
    "\n",
    "print('Building Promoter vocabulary')\n",
    "prom_strings = make_vocab_strings(prom_list_strings)\n",
    "prom_vocab = build_vocabulary(prom_strings, stop_GS)\n",
    "\n",
    "print('Building Passive vocabulary')\n",
    "pass_strings = make_vocab_strings(pass_list_strings)\n",
    "pass_vocab = build_vocabulary(pass_strings, stop_GS)\n",
    "\n",
    "print('Building Detractor vocabulary')\n",
    "det_strings = make_vocab_strings(det_list_strings)\n",
    "det_vocab = build_vocabulary(det_strings, stop_GS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = 'geek' in stop_GS\n",
    "print(f'{gs}')\n",
    "\n",
    "gs = 'geek' in prom_vocab\n",
    "print(f'{gs}')\n",
    "prom_vocab.update(['geek', 'squad', 'service', 'customer', 'service', 'xyxyxz','\\''])\n",
    "\n",
    "gs = 'geek' in prom_vocab\n",
    "print(f'{gs}')\n",
    "\n",
    "gs = 'geek' in pass_vocab\n",
    "print(f'{gs}')\n",
    "pass_vocab.update(['geek', 'squad', 'service', 'customer', 'service', 'xyxyxz','\\''])\n",
    "\n",
    "det_vocab.update(['geek', 'squad', 'service', 'customer', 'service', 'xyxyxz','\\''])\n",
    "\n",
    "gs = 'geek' in det_vocab\n",
    "print(f'{gs}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promoters only\n",
    "Freqs_prom = nps_freqs(prom_list_strings)\n",
    "#Freqs_prom = nps_freqs(prom_strings)\n",
    "\n",
    "Freqs_prom.plot(20)\n",
    "topWords_prom = Freqs_prom.most_common(10)\n",
    "\n",
    "# Freqs_prom.tabulate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passives only\n",
    "Freqs_pass = nps_freqs(pass_list_strings)\n",
    "Freqs_pass.plot(10)\n",
    "topWords_pass = Freqs_pass.most_common(20)\n",
    "\n",
    "Freqs_pass.tabulate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detractors only\n",
    "Freqs_det = nps_freqs(det_list_strings)\n",
    "Freqs_det.plot(10)\n",
    "topWords_det = Freqs_det.most_common(25)\n",
    "\n",
    "#Freqs_det.tabulate(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "#im = Image.open(\"promoters1.png\")\n",
    "#bg_pic = np.asarray(im)\n",
    "\n",
    "\n",
    "wordcloud_prom = WordCloud(width = 1280, height = 1024,\n",
    "                #mask=bg_pic,\n",
    "                background_color ='white',\n",
    "                contour_color='black',\n",
    "                contour_width=1,\n",
    "                stopwords = stop_GS,\n",
    "                collocations=False,\n",
    "                min_font_size = 4).generate(prom_strings)\n",
    "wordcloud_prom.to_file('promoters.png')\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (12, 8), facecolor = None)\n",
    "plt.imshow(wordcloud_prom)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_pass = WordCloud(width = 1280, height = 1024,\n",
    "                background_color ='white',\n",
    "                stopwords = stop_GS,\n",
    "                collocations=False,\n",
    "                min_font_size = 4).generate(prom_strings)\n",
    "\n",
    "wordcloud_prom.to_file('passive.png')\n",
    "\n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (12, 8), facecolor = None)\n",
    "plt.imshow(wordcloud_pass)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_det = WordCloud(width = 1280, height = 1024,\n",
    "                background_color ='white',\n",
    "                stopwords = stop_GS,\n",
    "                collocations=True,\n",
    "                min_font_size = 4).generate(det_strings)\n",
    "wordcloud_det.to_file('detractors.png')\n",
    "\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (12, 8), facecolor = None)\n",
    "plt.imshow(wordcloud_det)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Bigrams across Promoters/Passives/Detractors\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "fourgram_measures = nltk.collocations.QuadgramAssocMeasures()\n",
    "\n",
    "topnumb = 10\n",
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update(['best', 'buy', 'geek', 'squad', 'customer', 'service', 'xyxyxz','\\''])\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "\n",
    "words = \" \".join(str(comment).lower() for comment in prom_df.NPSCommentCleaned)\n",
    "# wt = word_tokenize(words)\n",
    "wt = nltk.wordpunct_tokenize(words)\n",
    "\n",
    "# bigrams - promoters\n",
    "bcf_prom = BigramCollocationFinder.from_words(wt)\n",
    "bcf_prom.apply_word_filter(filter_stops)\n",
    "bigrams_proms = bcf_prom.nbest(BigramAssocMeasures.likelihood_ratio, topnumb)\n",
    "scored_bigrams_proms = bcf_prom.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "# trigrams - promoters\n",
    "tcf_prom = TrigramCollocationFinder.from_words(wt)\n",
    "tcf_prom.apply_word_filter(filter_stops)\n",
    "trigrams_proms = tcf_prom.nbest(TrigramAssocMeasures.likelihood_ratio, topnumb)\n",
    "\n",
    "# bigrams - passives\n",
    "words2 = \" \".join(str(comment).lower() for comment in pass_df.NPSCommentCleaned)\n",
    "wt = word_tokenize(words2)\n",
    "bcf_pass = BigramCollocationFinder.from_words(wt)\n",
    "bcf_pass.apply_word_filter(filter_stops)\n",
    "bigrams_pass = bcf_pass.nbest(BigramAssocMeasures.likelihood_ratio, topnumb)\n",
    "\n",
    "# trigrams - passives\n",
    "tcf_pass = TrigramCollocationFinder.from_words(wt)\n",
    "tcf_pass.apply_word_filter(filter_stops)\n",
    "trigrams_pass = tcf_pass.nbest(TrigramAssocMeasures.likelihood_ratio, topnumb)\n",
    "\n",
    "# bigrams - detractors\n",
    "words3 = \" \".join(str(comment).lower() for comment in det_df.NPSCommentCleaned)\n",
    "wt = word_tokenize(words3)\n",
    "bcf_det = BigramCollocationFinder.from_words(wt)\n",
    "bcf_det.apply_word_filter(filter_stops)\n",
    "bigrams_det = bcf_det.nbest(BigramAssocMeasures.likelihood_ratio, topnumb)\n",
    "\n",
    "# trigrams - detractors\n",
    "tcf_det = TrigramCollocationFinder.from_words(wt)\n",
    "tcf_det.apply_word_filter(filter_stops)\n",
    "trigrams_det = tcf_det.nbest(TrigramAssocMeasures.likelihood_ratio, topnumb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ngram(ngrams, label):\n",
    "    print(f'{label}')\n",
    "    for ng in ngrams:\n",
    "        wordstring = ''\n",
    "        for w in ng:\n",
    "            wordstring += str(w)\n",
    "            wordstring += \" \"\n",
    "        print(wordstring)\n",
    "    return\n",
    "\n",
    "print_ngram(bigrams_proms, \"Promoter Bigrams: \")\n",
    "print('-------------')\n",
    "print_ngram(trigrams_proms, \"Promoter Trigrams: \")\n",
    "print('*************')\n",
    "print('')\n",
    "\n",
    "print_ngram(bigrams_pass, \"Passive Bigrams: \")\n",
    "print('-------------')\n",
    "print_ngram(trigrams_pass, \"Passive Trigrams: \")\n",
    "print('*************')\n",
    "print('')\n",
    "\n",
    "print_ngram(bigrams_det, \"Detractor Bigrams: \")\n",
    "print('-------------')\n",
    "print_ngram(trigrams_det, \"Detractor Trigrams: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ee8fbf491c2dc09f1850242389295773ac6375f10ce9efdb2142efbd90a9151"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('aigpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
