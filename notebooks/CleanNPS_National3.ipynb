{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Purpose: clean up raw NPS .xlsx files prior to training, write cleaned output to new .csv file\n",
    "# Method: \n",
    "#   1) For each store... Remove missing values, clean text, remove punctuation, lemmatize\n",
    "#   2) Use textblob to calculate sentiment scores for NPS and Overall comments\n",
    "#   3) Add sentiment scores for both the NPS Comment, and Overall Comment.\n",
    "#   4) Combine all store comments into single aggregated file by district\n",
    "#\n",
    "# Author: Eric G. Suchanek, PhD\n",
    "# (c)2022 BestBuy, all rights reserved\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import os\n",
    "\n",
    "# Bestbuy specifics\n",
    "import bby\n",
    "import bby.util as ut\n",
    "\n",
    "from bby.util import clean_doc, tb_enrich, nps_cleanstring\n",
    "from bby.util import lemma_remove_stopwords, get_wordnet_pos, nps_lemmatise\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "_territory = ut.Our_Territory\n",
    "_market = ut.Our_Market\n",
    "_district = ut.Our_District\n",
    "null_str = \"xyxyxz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert 'promoter' 'passive' 'detractor' to numerical index\n",
    "def nps_to_code(nps_list):\n",
    "    codelist = []\n",
    "    for comment in nps_list:\n",
    "        if (comment ==\"Promoter\"):\n",
    "            codelist.append('2')\n",
    "        elif (comment == \"Passive\"):\n",
    "            codelist.append('1')\n",
    "        elif (comment == \"Detractor\"):\n",
    "            codelist.append('0')\n",
    "        else:\n",
    "            codelist.append('xxx')\n",
    "    return(codelist)\n",
    "\n",
    "\n",
    "# write the comments (nps and overall) to files in specific directories based on whether they are\n",
    "# promoters, passive or detractors\n",
    "# write the list of comments to a file based on the path, district, and prefix\n",
    "def write_sentences(sentencelist, storename, _path, district, prefix):\n",
    "    lc = 1\n",
    "    outfilename = f'{_path}{district}_{storename}_{prefix}_{lc}{ut._txt_extension}'\n",
    "    outfile = open(outfilename, 'w')\n",
    "\n",
    "    for items in sentencelist:\n",
    "        outfile.writelines(items)\n",
    "        outfile.write('\\n')\n",
    "        lc += 1\n",
    "    outfile.close()\n",
    "    return\n",
    "\n",
    "def write_comments(district, storelist):\n",
    "    DEBUG = ut.DEBUG\n",
    "\n",
    "    if (DEBUG):\n",
    "        print(storelist)\n",
    "\n",
    "    for storename in storelist:\n",
    "        if (DEBUG):\n",
    "            print(f'....Writing comments for store: {storename}')\n",
    "        input_filename = f\"{ut._cleaned_path}{ut._output_filename_prefix}{storename}_{district}.csv\"\n",
    "        new_df = pd.read_csv(input_filename)\n",
    "\n",
    "        pos_comments = new_df.loc[new_df['NPS_Code'] == 2]\n",
    "        pass_comments = new_df.loc[new_df['NPS_Code'] == 1]\n",
    "        detr_comments = new_df.loc[new_df['NPS_Code'] == 0]\n",
    "\n",
    "        pos_stringlist = pos_comments['NPSCommentCleaned']\n",
    "        pass_stringlist = pass_comments['NPSCommentCleaned']\n",
    "        detr_stringlist = detr_comments['NPSCommentCleaned']\n",
    "        \n",
    "        write_sentences(pos_stringlist, storename, ut._prom_words_path, district, \"NPSComment\")\n",
    "        write_sentences(pass_stringlist, storename, ut._pass_words_path, district, \"NPSComment\")\n",
    "        write_sentences(detr_stringlist, storename, ut._det_words_path, district, \"NPSComment\")\n",
    "\n",
    "        pos_stringlist = pos_comments['OverallCommentCleaned']\n",
    "        pass_stringlist = pass_comments['OverallCommentCleaned']\n",
    "        detr_stringlist = detr_comments['OverallCommentCleaned']\n",
    "\n",
    "        write_sentences(pos_stringlist, storename, ut._prom_words_path, district, \"OverallComment\")\n",
    "        write_sentences(pass_stringlist, storename, ut._pass_words_path, district, \"OverallComment\")\n",
    "        write_sentences(detr_stringlist, storename, ut._det_words_path, district, \"OverallComment\")\n",
    "    return\n",
    "\n",
    "# NPS spreadsheet cleanup and reformatting for natural language processing\n",
    "# data paths and filename patterns\n",
    "# Assumes following directory structure\n",
    "# main directory/\n",
    "#  - notebook/\n",
    "#  - data/\n",
    "#  - raw/\n",
    "#   -clean/\n",
    "\n",
    "# Given the district, read the raw .xlsx file, fill null values, and perform the following cleanup:\n",
    "#  - remove URL, emails, extraneous punctuation\n",
    "#  - calculate sentiment scores from the blob package\n",
    "# write a new .csv file with NPS comment, overall comment, confirm number, location, workforce, NPS rating\n",
    "#\n",
    "\n",
    "\n",
    "def NPS_cleanup_market(market=_market, district=_district):\n",
    "    print(f'Processing market: {market}')\n",
    "    NPS_cleanup_district(market, district)\n",
    "    return\n",
    "\n",
    "# will need to change this to work like NPS_cleanup_national since we now use NPS_cleanup_national primarily\n",
    "# 5/23/22 -egs-\n",
    "#\n",
    "def NPS_cleanup_district(market, district):   \n",
    "    storelist1 = ut.district_stores_dict.get(market)\n",
    "    storelist = storelist1.get(district)\n",
    "    _cleaned_path = ut._cleaned_path\n",
    "    _raw_path = ut._raw_path\n",
    "    _output_filename_prefix = ut._output_filename_prefix\n",
    "    _filename_prefix = ut._filename_prefix\n",
    "    \n",
    "    # filename for aggregated final file\n",
    "    cleaned_filename = f\"{_cleaned_path}NPS_District_{district}.csv\"\n",
    "    output_filename_list = []\n",
    "\n",
    "    print(f'..Processing District: {district}')\n",
    "    \n",
    "    for storename in storelist:\n",
    "        all_promotors_list = []\n",
    "        all_passive_list = []\n",
    "        all_detractor_list = []\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "        input_filename = f\"{_raw_path}{_filename_prefix}{storename}.xlsx\"\n",
    "        output_filename = f\"{_cleaned_path}{_output_filename_prefix}{storename}_{district}.csv\"\n",
    "        output_filename_list.append(output_filename)\n",
    "        storename_string = f'_{storename}'\n",
    "\n",
    "        # read the .xlsx file, skipping the first 4 lines since they are not the\n",
    "        # actual column headers\n",
    "        print(f'...Processing Store: {storename}')\n",
    "        all_df = pd.read_excel(input_filename, header=3)\n",
    "        \n",
    "        # Fill null values.\n",
    "        # drop missing NPS Comments\n",
    "        # df.dropna(subset=['name', 'toy'])\n",
    "        \n",
    "        #all_df.dropna(subset=['NPS® Comment'])\n",
    "        all_df['Overall Comment'].fillna(null_str, inplace = True)\n",
    "        all_df['ConfirmationNumber'].fillna(\"0000\", inplace = True)\n",
    "        all_df['Service Order ID'].fillna(\"0000-0000\", inplace = True)\n",
    "        all_df['Location'] = storename_string\n",
    "\n",
    "        # Map the 'promoter', 'passive', 'detractor' strings to 2, 1, 0 in order to encode the\n",
    "        # NPS overall sentiment.\n",
    "        # Define how we want to change the label name\n",
    "        # label_map = {'Detractor': 0, 'Passive': 1, 'Promoter':2}\n",
    "        # tmp_df = all_df['NPS® Breakdown']\n",
    "        # Excute the label change, replacing the NPS Breakdown string with the integer\n",
    "        # tmp_df.replace({'NPS® Breakdown': label_map}, inplace=True)\n",
    "\n",
    "\n",
    "        # now create the new_df with appropriate columns\n",
    "        NPS_code = nps_to_code(all_df['NPS® Breakdown'].values.tolist())\n",
    "        \n",
    "        new_df = all_df[['Location','Workforce','NPS® Breakdown']].copy()\n",
    "        new_df['NPS_Code'] = NPS_code\n",
    "\n",
    "        # Splitting pd.Series to list to perform the sentiment analysis and\n",
    "        # text cleanup\n",
    "\n",
    "        temp = []\n",
    "        data_to_list = all_df['NPS® Comment']\n",
    "        \n",
    "        for i in range(len(data_to_list)):\n",
    "            clean_string = str(clean_doc(data_to_list[i]))\n",
    "            temp.append(lemma_remove_stopwords(clean_string))\n",
    "        \n",
    "        new_df['NPSCommentCleaned'] = temp\n",
    "        new_df['NPSCommentPolarity'], new_df['NPSCommentSubjectivity'] = tb_enrich(temp)\n",
    "        \n",
    "        temp = []\n",
    "        data_to_list = all_df['Overall Comment']\n",
    "        \n",
    "        for i in range(len(data_to_list)):\n",
    "            clean_string = str(clean_doc(clean_string))\n",
    "            temp.append(lemma_remove_stopwords(clean_string))\n",
    "        new_df['OverallCommentCleaned'] = temp\n",
    "        \n",
    "        OverallCommentPolarity, OverallCommentSubjectivity = tb_enrich(temp)\n",
    "        new_df['OverallCommentPolarity'], new_df['OverallCommentSubjectivity'] = OverallCommentPolarity, OverallCommentSubjectivity\n",
    "        \n",
    "        # write the store file\n",
    "        new_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    # now concatenate all of the resulting files into a single district .csv file\n",
    "    total_df = pd.DataFrame()\n",
    "    for infile in output_filename_list:\n",
    "        df = pd.read_csv(infile)\n",
    "        total_df = pd.concat([total_df, df], axis=0)\n",
    "        #os.remove(infile)\n",
    "\n",
    "    print(f'..Writing {cleaned_filename} with a total of {total_df.shape[0]} comments')\n",
    "    total_df.to_csv(cleaned_filename, index=False)\n",
    "    \n",
    "    write_comments(district, storelist)\n",
    "    \n",
    "    print('Done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall routine to clean raw NPS .xls file\n",
    "# Grab only columns we need. Remove stop words, lemmatize and stem NPS comments\n",
    "# Drop comments with a null comment string, write the file to the ../clean data directory\n",
    "\n",
    "from bby.util import lemma_remove_stopwords\n",
    "import time\n",
    "\n",
    "def NPS_cleanup_national(_input_filename, _output_filename, skip=3):\n",
    "    output_filename = _output_filename\n",
    "    input_filename = _input_filename\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    # read the .xlsx file, skipping the first 4 lines since they are not the\n",
    "    # actual column headers\n",
    "      \n",
    "    all_df = pd.read_excel(input_filename, header=skip) # is 3 for real file, 4 for my short one\n",
    "    total_comments = all_df.shape[0] # original number of comments\n",
    "\n",
    "    print(f'....Read {total_comments} comments from National file {input_filename}.')\n",
    "    \n",
    "    # Fill null values.\n",
    "    all_df['NPS® Comment'].fillna(null_str, inplace = True)\n",
    "    all_df['Overall Comment'].fillna(null_str, inplace = True)\n",
    "\n",
    "    '''\n",
    "    all_df['ConfirmationNumber'].fillna(\"0000\", inplace = True)\n",
    "    all_df['Service Order ID'].fillna(\"0000-0000\", inplace = True)\n",
    "    '''\n",
    "\n",
    "    all_df['Location'] = all_df['LOC ID'].copy()\n",
    "\n",
    "    NPS_code = nps_to_code(all_df['NPS® Breakdown'].values.tolist())\n",
    "    \n",
    "    new_df = all_df[['Location','Workforce','NPS® Breakdown', 'respid2']].copy()\n",
    "    new_df['NPS_Code'] = NPS_code\n",
    "    \n",
    "    nps_list = all_df['NPS® Comment'] \n",
    "    new_df['NPSCommentCleaned'] = nps_list.apply(nps_cleanstring)\n",
    "    new_df['NPSCommentLemmatised'] = nps_list.apply(nps_lemmatise)\n",
    "    \n",
    "    temp = new_df['NPSCommentCleaned'].values.tolist()\n",
    "    new_df['NPSCommentPolarity'], new_df['NPSCommentSubjectivity'] = tb_enrich(temp)\n",
    "    \n",
    "    overall_list = all_df['Overall Comment']\n",
    "    new_df['OverallCommentCleaned'] = overall_list.apply(nps_cleanstring)\n",
    "    new_df['OverallCommentLemmatised'] = overall_list.apply(nps_lemmatise)\n",
    "\n",
    "    temp = new_df['OverallCommentCleaned'].values.tolist()\n",
    "    OverallCommentPolarity, OverallCommentSubjectivity = tb_enrich(temp)\n",
    "\n",
    "    new_df['OverallCommentPolarity'], new_df['OverallCommentSubjectivity'] = OverallCommentPolarity, OverallCommentSubjectivity\n",
    "\n",
    "    null_comments = new_df[new_df['NPSCommentCleaned'] == null_str].values.tolist()\n",
    "\n",
    "    print(f'There were {len(null_comments)} Null Promoter comments.')\n",
    "    print(f'Before drop: {new_df.shape[0]}')\n",
    "\n",
    "    # drop any null comments\n",
    "    new_df = new_df[new_df['NPSCommentCleaned'] != null_str]\n",
    "    dropped_comments = new_df.shape[0]\n",
    "    \n",
    "    print(f'Dropped: {total_comments - dropped_comments} null NPS comments')\n",
    "    \n",
    "    # write the file file\n",
    "    new_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print('Done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall routine to clean raw NPS .xls file\n",
    "# Grab only columns we need. Remove stop words, lemmatize and stem NPS comments\n",
    "# Drop comments with a null comment string, write the file to the ../clean data directory\n",
    "\n",
    "from bby.util import lemma_remove_stopwords\n",
    "import time\n",
    "\n",
    "def NPS_cleanup_uso(_input_filename, _output_filename, skip=4):\n",
    "    _cleaned_path = ut._cleaned_path\n",
    "    _raw_path = ut._raw_path\n",
    "    \n",
    "    # filename for aggregated final file\n",
    "    output_filename = _output_filename\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    input_filename = _input_filename\n",
    "\n",
    "    print(f'..Processing National NPS Surveys from {input_filename}')\n",
    "\n",
    "    # read the .xlsx file, skipping the first 4 lines since they are not the\n",
    "    # actual column headers\n",
    "    \n",
    "    all_df = pd.read_excel(input_filename, header=skip) # is 3 for real file, 4 for my short one\n",
    "    total_comments = all_df.shape[0] # original number of comments\n",
    "\n",
    "    print(f'....Read {total_comments} comments from National file {input_filename}')\n",
    "    \n",
    "    # Fill null values.\n",
    "    all_df['NPS® Comment'].fillna(null_str, inplace = True)\n",
    "    all_df['Overall Comment'].fillna(null_str, inplace = True)\n",
    "    all_df['Location'] = all_df['Store ID'].copy()\n",
    "\n",
    "    NPS_code = nps_to_code(all_df['NPS® Breakdown'].values.tolist())\n",
    "    \n",
    "    new_df = all_df[['Location','Workforce','NPS® Breakdown', 'responseid']].copy()\n",
    "    new_df['NPS_Code'] = NPS_code\n",
    "    \n",
    "    nps_list = all_df['NPS® Comment'] \n",
    "    new_df['NPSCommentCleaned'] = nps_list.apply(nps_cleanstring)\n",
    "    new_df['NPSCommentLemmatised'] = nps_list.apply(nps_lemmatise)\n",
    "    \n",
    "    temp = new_df['NPSCommentCleaned'].values.tolist()\n",
    "    new_df['NPSCommentPolarity'], new_df['NPSCommentSubjectivity'] = tb_enrich(temp)\n",
    "    \n",
    "    overall_list = all_df['Overall Comment']\n",
    "    new_df['OverallCommentCleaned'] = overall_list.apply(nps_cleanstring)\n",
    "    new_df['OverallCommentLemmatised'] = overall_list.apply(nps_lemmatise)\n",
    "\n",
    "    temp = new_df['OverallCommentCleaned'].values.tolist()\n",
    "    OverallCommentPolarity, OverallCommentSubjectivity = tb_enrich(temp)\n",
    "\n",
    "    new_df['OverallCommentPolarity'], new_df['OverallCommentSubjectivity'] = OverallCommentPolarity, OverallCommentSubjectivity\n",
    "\n",
    "    null_comments = new_df[new_df['NPSCommentCleaned'] == null_str].values.tolist()\n",
    "\n",
    "    print(f'There were {len(null_comments)} Null Promoter comments.')\n",
    "    print(f'Before drop: {new_df.shape[0]}')\n",
    "\n",
    "    # drop any null comments\n",
    "    new_df = new_df[new_df['NPSCommentCleaned'] != null_str]\n",
    "    dropped_comments = new_df.shape[0]\n",
    "    \n",
    "    print(f'Dropped: {total_comments - dropped_comments} null NPS comments')\n",
    "    \n",
    "    # write the file file\n",
    "    new_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print('Done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:42.334657Z",
     "iopub.status.busy": "2020-10-06T21:19:42.333827Z",
     "iopub.status.idle": "2020-10-06T21:19:42.337209Z",
     "shell.execute_reply": "2020-10-06T21:19:42.337696Z"
    },
    "papermill": {
     "duration": 0.04812,
     "end_time": "2020-10-06T21:19:42.337823",
     "exception": false,
     "start_time": "2020-10-06T21:19:42.289703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# since we have many more promoters than either passives or detractors\n",
    "# we bias toward promoters. Let's equalize the distribution by grabbing the same number of promoters\n",
    "# as passives or detractors.\n",
    "# \n",
    "# equalized comment numbers data resulting from CleanNPS procedure/process\n",
    "#\n",
    "\n",
    "def equalize_NPS_file(input_filename, output_filename):\n",
    "    NPS_df = pd.read_csv(input_filename)\n",
    "    \n",
    "    prom_list_mask = NPS_df['NPS_Code'] == 2\n",
    "    pass_list_mask = NPS_df['NPS_Code'] == 1\n",
    "    det_list_mask = NPS_df['NPS_Code'] == 0\n",
    "\n",
    "    prom_list = NPS_df[prom_list_mask]\n",
    "    pass_list = NPS_df[pass_list_mask]\n",
    "    det_list = NPS_df[det_list_mask]\n",
    "\n",
    "    prom_list_len = prom_list.shape[0]\n",
    "    pass_list_len = pass_list.shape[0]\n",
    "    det_list_len = det_list.shape[0]\n",
    "    \n",
    "    # Rechecking balance of target classes after equalization\n",
    "    sentiments = list(NPS_df[\"NPS® Breakdown\"].unique())\n",
    "    sentiment_nums = [len(NPS_df[NPS_df[\"NPS® Breakdown\"] == sentiment]) / len(NPS_df) for sentiment in sentiments]\n",
    "\n",
    "    print (f'Before equalization, Promoters: {prom_list_len}, Passives: {pass_list_len}, Detractors: {det_list_len}')\n",
    "    plt.bar(sentiments, sentiment_nums)\n",
    "\n",
    "    NPS_df = pd.DataFrame()\n",
    "\n",
    "    # since we normally have many more promoters than passive/detractors we should normalize our distribution\n",
    "    # sample an appropriate number of promoters to equalize distribution\n",
    "    prom_sample_size = (pass_list_len + det_list_len) // 2\n",
    "    prom_list = prom_list.sample(prom_sample_size)\n",
    "\n",
    "    NPS_df = pd.DataFrame()\n",
    "    NPS_df = pd.concat([prom_list, pass_list, det_list])\n",
    "    NPS_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    # Rechecking balance of target classes after equalization\n",
    "    sentiments = list(NPS_df[\"NPS® Breakdown\"].unique())\n",
    "    sentiment_nums = [len(NPS_df[NPS_df[\"NPS® Breakdown\"] == sentiment]) / len(NPS_df) for sentiment in sentiments]\n",
    "\n",
    "    print (f'After redistribution: Promoters: {prom_sample_size}, Passives: {pass_list_len}, Detractors: {det_list_len}')\n",
    "    plt.bar(sentiments, sentiment_nums)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing National NPS file at 2022-06-20 09:17:00.698363\n",
      "....Read 153223 comments from National file ../data/raw/NPS_NATL_archive.xlsx.\n",
      "There were 26567 Null Promoter comments.\n",
      "Before drop: 153223\n",
      "Dropped: 26567 null NPS comments\n",
      "Done\n",
      "Total elapsed time was: 0:04:05.847211\n",
      "Before equalization, Promoters: 108854, Passives: 7785, Detractors: 10017\n",
      "After redistribution: Promoters: 8901, Passives: 7785, Detractors: 10017\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We are in Territory 3, Market 21, District 125 - national cleanup takes about 10 minutes\n",
    "# NPS_cleanup_market(21, 125)\n",
    "\n",
    "# Since we can do national extracts it's easier just to process the entire file.\n",
    "# Ideally we'd have all the stores mapped to their relevant Markets and Districts... <tbd>\n",
    "# by convention I am putting raw files in ../data/raw and cleaned files in ../data/cleaned\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print(f'Starting processing National NPS file at {start}')\n",
    "\n",
    "NPS_cleanup_national('../data/raw/NPS_NATL_archive.xlsx', '../data/clean/NPS_Natl_cleaned.csv', 0)\n",
    "#NPS_cleanup_national('../data/raw/export_Main Hierarchy_NTL.xlsx', '../data/clean/NPS_Natl_cleaned.csv')\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print(f'Total elapsed time was: {end - start}')\n",
    "equalize_NPS_file('../data/clean/NPS_NATL_cleaned.csv', '../data/clean/NPS_NATL_subset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing National USO NPS file at 2022-06-14 13:56:49.416078\n",
      "..Processing National NPS Surveys from ../data/raw/export_USO Hierarchy_NTL.xlsx\n",
      "....Read 98513 comments from National file ../data/raw/export_USO Hierarchy_NTL.xlsx\n",
      "There were 18292 Null Promoter comments.\n",
      "Before drop: 98513\n",
      "Dropped: 18292 null NPS comments\n",
      "Done\n",
      "Before equalization, Promoters: 72767, Passives: 3507, Detractors: 3947\n",
      "After redistribution: Promoters: 3727, Passives: 3507, Detractors: 3947\n",
      "Total elapsed time was: 0:02:27.023602\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "print(f'Starting processing National USO NPS file at {datetime.datetime.now()}')\n",
    "NPS_cleanup_uso('../data/raw/export_USO Hierarchy_NTL.xlsx', '../data/clean/NPS_USO_cleaned.csv', 6)\n",
    "end = datetime.datetime.now()\n",
    "print(f'Total elapsed time was: {end - start}')\n",
    "equalize_NPS_file('../data/clean/NPS_USO_cleaned.csv', '../data/clean/NPS_USO_subset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2ae5d3a08a6e204c8c8cee8069ee9bbd2ab88ccb9eee13930db12104613cdbd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
