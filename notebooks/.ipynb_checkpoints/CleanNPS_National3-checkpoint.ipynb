{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Purpose: clean up raw NPS .xlsx files prior to training, write cleaned output to new .csv file\n",
    "# Method: \n",
    "#   1) For each store... Remove missing values, clean text, remove punctuation, lemmatize\n",
    "#   2) Use textblob to calculate sentiment scores for NPS and Overall comments\n",
    "#   3) Add sentiment scores for both the NPS Comment, and Overall Comment.\n",
    "#   4) Combine all store comments into single aggregated file by district\n",
    "#\n",
    "# Author: Eric G. Suchanek, PhD\n",
    "# (c)2022 BestBuy, all rights reserved\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "#TextBlob Features\n",
    "from textblob import TextBlob\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Bestbuy specifics\n",
    "import bby\n",
    "import bby.util as ut\n",
    "\n",
    "from bby.util import clean_doc, tb_enrich, nps_cleanstring\n",
    "from bby.util import lemma_remove_stopwords, get_wordnet_pos, nps_lemmatise\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "_territory = ut.Our_Territory\n",
    "_market = ut.Our_Market\n",
    "_district = ut.Our_District\n",
    "null_str = \"xyxyxz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert 'promoter' 'passive' 'detractor' to numerical index\n",
    "def nps_to_code(nps_list):\n",
    "    codelist = []\n",
    "    for comment in nps_list:\n",
    "        if (comment ==\"Promoter\"):\n",
    "            codelist.append('2')\n",
    "        elif (comment == \"Passive\"):\n",
    "            codelist.append('1')\n",
    "        elif (comment == \"Detractor\"):\n",
    "            codelist.append('0')\n",
    "        else:\n",
    "            codelist.append('xxx')\n",
    "    return(codelist)\n",
    "\n",
    "\n",
    "# write the comments (nps and overall) to files in specific directories based on whether they are\n",
    "# promoters, passive or detractors\n",
    "# write the list of comments to a file based on the path, district, and prefix\n",
    "def write_sentences(sentencelist, storename, _path, district, prefix):\n",
    "    lc = 1\n",
    "    outfilename = f'{_path}{district}_{storename}_{prefix}_{lc}{ut._txt_extension}'\n",
    "    outfile = open(outfilename, 'w')\n",
    "\n",
    "    for items in sentencelist:\n",
    "        outfile.writelines(items)\n",
    "        outfile.write('\\n')\n",
    "        lc += 1\n",
    "    outfile.close()\n",
    "    return\n",
    "\n",
    "def write_comments(district, storelist):\n",
    "    DEBUG = ut.DEBUG\n",
    "\n",
    "    if (DEBUG):\n",
    "        print(storelist)\n",
    "\n",
    "    for storename in storelist:\n",
    "        if (DEBUG):\n",
    "            print(f'....Writing comments for store: {storename}')\n",
    "        input_filename = f\"{ut._cleaned_path}{ut._output_filename_prefix}{storename}_{district}.csv\"\n",
    "        new_df = pd.read_csv(input_filename)\n",
    "\n",
    "        pos_comments = new_df.loc[new_df['NPS_Code'] == 2]\n",
    "        pass_comments = new_df.loc[new_df['NPS_Code'] == 1]\n",
    "        detr_comments = new_df.loc[new_df['NPS_Code'] == 0]\n",
    "\n",
    "        pos_stringlist = pos_comments['NPSCommentCleaned']\n",
    "        pass_stringlist = pass_comments['NPSCommentCleaned']\n",
    "        detr_stringlist = detr_comments['NPSCommentCleaned']\n",
    "        \n",
    "        write_sentences(pos_stringlist, storename, ut._prom_words_path, district, \"NPSComment\")\n",
    "        write_sentences(pass_stringlist, storename, ut._pass_words_path, district, \"NPSComment\")\n",
    "        write_sentences(detr_stringlist, storename, ut._det_words_path, district, \"NPSComment\")\n",
    "\n",
    "        pos_stringlist = pos_comments['OverallCommentCleaned']\n",
    "        pass_stringlist = pass_comments['OverallCommentCleaned']\n",
    "        detr_stringlist = detr_comments['OverallCommentCleaned']\n",
    "\n",
    "        write_sentences(pos_stringlist, storename, ut._prom_words_path, district, \"OverallComment\")\n",
    "        write_sentences(pass_stringlist, storename, ut._pass_words_path, district, \"OverallComment\")\n",
    "        write_sentences(detr_stringlist, storename, ut._det_words_path, district, \"OverallComment\")\n",
    "    return\n",
    "\n",
    "# NPS spreadsheet cleanup and reformatting for natural language processing\n",
    "# data paths and filename patterns\n",
    "# Assumes following directory structure\n",
    "# main directory/\n",
    "#  - notebook/\n",
    "#  - data/\n",
    "#  - raw/\n",
    "#   -clean/\n",
    "\n",
    "# Given the district, read the raw .xlsx file, fill null values, and perform the following cleanup:\n",
    "#  - remove URL, emails, extraneous punctuation\n",
    "#  - calculate sentiment scores from the blob package\n",
    "# write a new .csv file with NPS comment, overall comment, confirm number, location, workforce, NPS rating\n",
    "#\n",
    "\n",
    "\n",
    "def NPS_cleanup_market(market=_market, district=_district):\n",
    "    print(f'Processing market: {market}')\n",
    "    NPS_cleanup_district(market, district)\n",
    "    return\n",
    "\n",
    "# will need to change this to work like NPS_cleanup_national since we now use NPS_cleanup_national primarily\n",
    "# 5/23/22 -egs-\n",
    "#\n",
    "def NPS_cleanup_district(market, district):   \n",
    "    storelist1 = ut.district_stores_dict.get(market)\n",
    "    storelist = storelist1.get(district)\n",
    "    _cleaned_path = ut._cleaned_path\n",
    "    _raw_path = ut._raw_path\n",
    "    _output_filename_prefix = ut._output_filename_prefix\n",
    "    _filename_prefix = ut._filename_prefix\n",
    "    \n",
    "    # filename for aggregated final file\n",
    "    cleaned_filename = f\"{_cleaned_path}NPS_District_{district}.csv\"\n",
    "    output_filename_list = []\n",
    "\n",
    "    print(f'..Processing District: {district}')\n",
    "    \n",
    "    for storename in storelist:\n",
    "        all_promotors_list = []\n",
    "        all_passive_list = []\n",
    "        all_detractor_list = []\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "        input_filename = f\"{_raw_path}{_filename_prefix}{storename}.xlsx\"\n",
    "        output_filename = f\"{_cleaned_path}{_output_filename_prefix}{storename}_{district}.csv\"\n",
    "        output_filename_list.append(output_filename)\n",
    "        storename_string = f'_{storename}'\n",
    "\n",
    "        # read the .xlsx file, skipping the first 4 lines since they are not the\n",
    "        # actual column headers\n",
    "        print(f'...Processing Store: {storename}')\n",
    "        all_df = pd.read_excel(input_filename, header=3)\n",
    "        \n",
    "        # Fill null values.\n",
    "        # drop missing NPS Comments\n",
    "        # df.dropna(subset=['name', 'toy'])\n",
    "        \n",
    "        #all_df.dropna(subset=['NPS® Comment'])\n",
    "        all_df['Overall Comment'].fillna(null_str, inplace = True)\n",
    "        all_df['ConfirmationNumber'].fillna(\"0000\", inplace = True)\n",
    "        all_df['Service Order ID'].fillna(\"0000-0000\", inplace = True)\n",
    "        all_df['Location'] = storename_string\n",
    "\n",
    "        # Map the 'promoter', 'passive', 'detractor' strings to 2, 1, 0 in order to encode the\n",
    "        # NPS overall sentiment.\n",
    "        # Define how we want to change the label name\n",
    "        # label_map = {'Detractor': 0, 'Passive': 1, 'Promoter':2}\n",
    "        # tmp_df = all_df['NPS® Breakdown']\n",
    "        # Excute the label change, replacing the NPS Breakdown string with the integer\n",
    "        # tmp_df.replace({'NPS® Breakdown': label_map}, inplace=True)\n",
    "\n",
    "\n",
    "        # now create the new_df with appropriate columns\n",
    "        NPS_code = nps_to_code(all_df['NPS® Breakdown'].values.tolist())\n",
    "        \n",
    "        new_df = all_df[['Location','Workforce','NPS® Breakdown']].copy()\n",
    "        new_df['NPS_Code'] = NPS_code\n",
    "\n",
    "        # Splitting pd.Series to list to perform the sentiment analysis and\n",
    "        # text cleanup\n",
    "\n",
    "        temp = []\n",
    "        data_to_list = all_df['NPS® Comment']\n",
    "        \n",
    "        for i in range(len(data_to_list)):\n",
    "            clean_string = str(clean_doc(data_to_list[i]))\n",
    "            temp.append(lemma_remove_stopwords(clean_string))\n",
    "        \n",
    "        new_df['NPSCommentCleaned'] = temp\n",
    "        new_df['NPSCommentPolarity'], new_df['NPSCommentSubjectivity'] = tb_enrich(temp)\n",
    "        \n",
    "        temp = []\n",
    "        data_to_list = all_df['Overall Comment']\n",
    "        \n",
    "        for i in range(len(data_to_list)):\n",
    "            clean_string = str(clean_doc(clean_string))\n",
    "            temp.append(lemma_remove_stopwords(clean_string))\n",
    "        new_df['OverallCommentCleaned'] = temp\n",
    "        \n",
    "        OverallCommentPolarity, OverallCommentSubjectivity = tb_enrich(temp)\n",
    "        new_df['OverallCommentPolarity'], new_df['OverallCommentSubjectivity'] = OverallCommentPolarity, OverallCommentSubjectivity\n",
    "        \n",
    "        # write the store file\n",
    "        new_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    # now concatenate all of the resulting files into a single district .csv file\n",
    "    total_df = pd.DataFrame()\n",
    "    for infile in output_filename_list:\n",
    "        df = pd.read_csv(infile)\n",
    "        total_df = pd.concat([total_df, df], axis=0)\n",
    "        #os.remove(infile)\n",
    "\n",
    "    print(f'..Writing {cleaned_filename} with a total of {total_df.shape[0]} comments')\n",
    "    total_df.to_csv(cleaned_filename, index=False)\n",
    "    \n",
    "    write_comments(district, storelist)\n",
    "    \n",
    "    print('Done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comment = \"this is a gar !, sentence, ERIC, ;@#$\"\n",
    "clean = nps_cleanstring(comment)\n",
    "print(f'Input: <{comment}>')\n",
    "print(f'Output: <{clean}>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall routine to clean raw NPS .xls file\n",
    "# Grab only columns we need. Remove stop words, lemmatize and stem NPS comments\n",
    "# Drop comments with a null comment string, write the file to the ../clean data directory\n",
    "\n",
    "from bby.util import lemma_remove_stopwords\n",
    "import time\n",
    "\n",
    "def NPS_cleanup_national(_input_filename, _output_filename, skip=3):\n",
    "    _cleaned_path = ut._cleaned_path\n",
    "    _raw_path = ut._raw_path\n",
    "    \n",
    "    # filename for aggregated final file\n",
    "    output_filename = f\"{_cleaned_path}{_output_filename}\"\n",
    "\n",
    "    # print(f'..Processing National NPS Surveys')\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    input_filename = f'{_raw_path}{_input_filename}'\n",
    "    #input_filename = f'{_raw_path}export_natl_small.xlsx'\n",
    "    \n",
    "    # read the .xlsx file, skipping the first 4 lines since they are not the\n",
    "    # actual column headers\n",
    "    \n",
    "    all_df = pd.read_excel(input_filename, header=skip) # is 3 for real file, 4 for my short one\n",
    "    total_comments = all_df.shape[0] # original number of comments\n",
    "\n",
    "    print(f'....Read {total_comments} comments from National file.')\n",
    "    \n",
    "    # Fill null values.\n",
    "    all_df['NPS® Comment'].fillna(null_str, inplace = True)\n",
    "    all_df['Overall Comment'].fillna(null_str, inplace = True)\n",
    "    all_df['ConfirmationNumber'].fillna(\"0000\", inplace = True)\n",
    "    all_df['Service Order ID'].fillna(\"0000-0000\", inplace = True)\n",
    "    all_df['Location'] = all_df['LOC ID'].copy()\n",
    "\n",
    "    NPS_code = nps_to_code(all_df['NPS® Breakdown'].values.tolist())\n",
    "    \n",
    "    new_df = all_df[['Location','Workforce','NPS® Breakdown', 'respid2']].copy()\n",
    "    new_df['NPS_Code'] = NPS_code\n",
    "    \n",
    "    nps_list = all_df['NPS® Comment'] \n",
    "    new_df['NPSCommentCleaned'] = nps_list.apply(nps_cleanstring)\n",
    "    new_df['NPSCommentLemmatised'] = nps_list.apply(nps_lemmatise)\n",
    "    \n",
    "    temp = new_df['NPSCommentCleaned'].values.tolist()\n",
    "    new_df['NPSCommentPolarity'], new_df['NPSCommentSubjectivity'] = tb_enrich(temp)\n",
    "    \n",
    "    overall_list = all_df['Overall Comment']\n",
    "    new_df['OverallCommentCleaned'] = overall_list.apply(nps_cleanstring)\n",
    "    new_df['OverallCommentLemmatised'] = overall_list.apply(nps_lemmatise)\n",
    "\n",
    "    temp = new_df['OverallCommentCleaned'].values.tolist()\n",
    "    OverallCommentPolarity, OverallCommentSubjectivity = tb_enrich(temp)\n",
    "\n",
    "    new_df['OverallCommentPolarity'], new_df['OverallCommentSubjectivity'] = OverallCommentPolarity, OverallCommentSubjectivity\n",
    "\n",
    "    null_comments = new_df[new_df['NPSCommentCleaned'] == null_str].values.tolist()\n",
    "\n",
    "    print(f'There were {len(null_comments)} Null Promoter comments.')\n",
    "    print(f'Before drop: {new_df.shape[0]}')\n",
    "\n",
    "    # drop any null comments\n",
    "    new_df = new_df[new_df['NPSCommentCleaned'] != null_str]\n",
    "    dropped_comments = new_df.shape[0]\n",
    "    \n",
    "    print(f'Dropped: {total_comments - dropped_comments} null NPS comments')\n",
    "    \n",
    "    # write the file file\n",
    "    new_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print('Done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall routine to clean raw NPS .xls file\n",
    "# Grab only columns we need. Remove stop words, lemmatize and stem NPS comments\n",
    "# Drop comments with a null comment string, write the file to the ../clean data directory\n",
    "\n",
    "from bby.util import lemma_remove_stopwords\n",
    "import time\n",
    "\n",
    "def NPS_cleanup_uso(_input_filename, _output_filename, skip=5):\n",
    "    _cleaned_path = ut._cleaned_path\n",
    "    _raw_path = ut._raw_path\n",
    "    \n",
    "    # filename for aggregated final file\n",
    "    output_filename = f\"{_cleaned_path}{_output_filename}\"\n",
    "\n",
    "    # print(f'..Processing National NPS Surveys')\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    input_filename = f'{_raw_path}{_input_filename}'\n",
    "    #input_filename = f'{_raw_path}export_natl_small.xlsx'\n",
    "    \n",
    "    # read the .xlsx file, skipping the first 4 lines since they are not the\n",
    "    # actual column headers\n",
    "    \n",
    "    all_df = pd.read_excel(_input_filename, header=skip) # is 3 for real file, 4 for my short one\n",
    "    total_comments = all_df.shape[0] # original number of comments\n",
    "\n",
    "    print(f'....Read {total_comments} comments from National file.')\n",
    "    \n",
    "    # Fill null values.\n",
    "    all_df['NPS® Comment'].fillna(null_str, inplace = True)\n",
    "    all_df['Overall Comment'].fillna(null_str, inplace = True)\n",
    "    all_df['Location'] = all_df['Store ID'].copy()\n",
    "\n",
    "    NPS_code = nps_to_code(all_df['NPS® Breakdown'].values.tolist())\n",
    "    \n",
    "    new_df = all_df[['Location','Workforce','NPS® Breakdown', 'responseid']].copy()\n",
    "    new_df['NPS_Code'] = NPS_code\n",
    "    \n",
    "    nps_list = all_df['NPS® Comment'] \n",
    "    new_df['NPSCommentCleaned'] = nps_list.apply(nps_cleanstring)\n",
    "    new_df['NPSCommentLemmatised'] = nps_list.apply(nps_lemmatise)\n",
    "    \n",
    "    temp = new_df['NPSCommentCleaned'].values.tolist()\n",
    "    new_df['NPSCommentPolarity'], new_df['NPSCommentSubjectivity'] = tb_enrich(temp)\n",
    "    \n",
    "    overall_list = all_df['Overall Comment']\n",
    "    new_df['OverallCommentCleaned'] = overall_list.apply(nps_cleanstring)\n",
    "    new_df['OverallCommentLemmatised'] = overall_list.apply(nps_lemmatise)\n",
    "\n",
    "    temp = new_df['OverallCommentCleaned'].values.tolist()\n",
    "    OverallCommentPolarity, OverallCommentSubjectivity = tb_enrich(temp)\n",
    "\n",
    "    new_df['OverallCommentPolarity'], new_df['OverallCommentSubjectivity'] = OverallCommentPolarity, OverallCommentSubjectivity\n",
    "\n",
    "    null_comments = new_df[new_df['NPSCommentCleaned'] == null_str].values.tolist()\n",
    "\n",
    "    print(f'There were {len(null_comments)} Null Promoter comments.')\n",
    "    print(f'Before drop: {new_df.shape[0]}')\n",
    "\n",
    "    # drop any null comments\n",
    "    new_df = new_df[new_df['NPSCommentCleaned'] != null_str]\n",
    "    dropped_comments = new_df.shape[0]\n",
    "    \n",
    "    print(f'Dropped: {total_comments - dropped_comments} null NPS comments')\n",
    "    \n",
    "    # write the file file\n",
    "    new_df.to_csv(_output_filename, index=False)\n",
    "    \n",
    "    print('Done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-06T21:19:42.334657Z",
     "iopub.status.busy": "2020-10-06T21:19:42.333827Z",
     "iopub.status.idle": "2020-10-06T21:19:42.337209Z",
     "shell.execute_reply": "2020-10-06T21:19:42.337696Z"
    },
    "papermill": {
     "duration": 0.04812,
     "end_time": "2020-10-06T21:19:42.337823",
     "exception": false,
     "start_time": "2020-10-06T21:19:42.289703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# since we have many more promoters than either passives or detractors\n",
    "# we bias toward promoters. Let's equalize the distribution by grabbing the same number of promoters\n",
    "# as passives or detractors.\n",
    "# \n",
    "# equalized comment numbers data resulting from CleanNPS procedure/process\n",
    "#\n",
    "\n",
    "def equalize_NPS_file(input_filename, output_filename):\n",
    "    NPS_df = pd.read_csv(input_filename)\n",
    "    \n",
    "    prom_list_mask = NPS_df['NPS_Code'] == 2\n",
    "    pass_list_mask = NPS_df['NPS_Code'] == 1\n",
    "    det_list_mask = NPS_df['NPS_Code'] == 0\n",
    "\n",
    "    prom_list = NPS_df[prom_list_mask]\n",
    "    pass_list = NPS_df[pass_list_mask]\n",
    "    det_list = NPS_df[det_list_mask]\n",
    "\n",
    "    prom_list_len = prom_list.shape[0]\n",
    "    pass_list_len = pass_list.shape[0]\n",
    "    det_list_len = det_list.shape[0]\n",
    "    \n",
    "    # Rechecking balance of target classes after equalization\n",
    "    sentiments = list(NPS_df[\"NPS® Breakdown\"].unique())\n",
    "    sentiment_nums = [len(NPS_df[NPS_df[\"NPS® Breakdown\"] == sentiment]) / len(NPS_df) for sentiment in sentiments]\n",
    "\n",
    "    print (f'Before equalization, Promoters: {prom_list_len}, Passives: {pass_list_len}, Detractors: {det_list_len}')\n",
    "    plt.bar(sentiments, sentiment_nums)\n",
    "\n",
    "    NPS_df = pd.DataFrame()\n",
    "\n",
    "    # since we normally have many more promoters than passive/detractors we should normalize our distribution\n",
    "    # sample an appropriate number of promoters to equalize distribution\n",
    "    prom_sample_size = (pass_list_len + det_list_len) // 2\n",
    "    prom_list = prom_list.sample(prom_sample_size)\n",
    "\n",
    "    NPS_df = pd.DataFrame()\n",
    "    NPS_df = pd.concat([prom_list, pass_list, det_list])\n",
    "    NPS_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    # Rechecking balance of target classes after equalization\n",
    "    sentiments = list(NPS_df[\"NPS® Breakdown\"].unique())\n",
    "    sentiment_nums = [len(NPS_df[NPS_df[\"NPS® Breakdown\"] == sentiment]) / len(NPS_df) for sentiment in sentiments]\n",
    "\n",
    "    print (f'After redistribution: Promoters: {prom_sample_size}, Passives: {pass_list_len}, Detractors: {det_list_len}')\n",
    "    plt.bar(sentiments, sentiment_nums)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing National NPS file at 2022-05-31 16:38:13.469148\n",
      "....Read 133644 comments from National file.\n"
     ]
    }
   ],
   "source": [
    "# We are in Market 21, District 125 - national cleanup takes about 10 minutes\n",
    "# NPS_cleanup_market(21, 125)\n",
    "\n",
    "# Since we can do national extracts it's easier just to process the entire file.\n",
    "# Ideally we'd have all the stores mapped to their relevant Markets and Districts... <tbd>\n",
    "# by convention I am putting raw files in ../data/raw and cleaned files in ../data/cleaned\n",
    "import datetime\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print(f'Starting processing National NPS file at {start}')\n",
    "\n",
    "# NPS_cleanup_national('export_Main Hierarchy_NTL.xlsx', 'NPS_Natl_cleaned.csv')\n",
    "# equalize_NPS_file('../data/clean/NPS_NATL_cleaned.csv', '../data/clean/NPS_NATL_subset.csv')\n",
    "\n",
    "NPS_cleanup_uso('export_USO Hierarchy_NTL.xlsx', 'NPS_USO_cleaned.csv')\n",
    "equalize_NPS_file('../data/clean/NPS_USO_cleaned.csv', '../data/clean/NPS_USO_subset.csv')\n",
    "\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "print(f'Total elapsed time was: {end - start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ee8fbf491c2dc09f1850242389295773ac6375f10ce9efdb2142efbd90a9151"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
